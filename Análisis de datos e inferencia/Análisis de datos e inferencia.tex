\documentclass{report}
\usepackage[spanish]{babel}
\usepackage{amssymb, amsmath, amsthm, hyperref, parskip}

\title{Análisis de datos e inferencia}
\author{}

\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{corollary}[theorem]{Corolario}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{proposition}[theorem]{Proposición}

\theoremstyle{remark}
\newtheorem*{remark}{Observación}
\theoremstyle{remark}
\newtheorem*{note}{Nota}
\theoremstyle{remark}
\newtheorem*{notation}{Notación}

\theoremstyle{definition}
\newtheorem{definition}{Definición}[chapter]
\theoremstyle{definition}
\newtheorem*{properties}{Propiedades}
\theoremstyle{definition}

\begin{document}
\maketitle
\tableofcontents

\chapter{Modelo de regresión lineal simple}
\section{Introducción}

La regresión lineal es un modelo matemático que nos permite establecer la relación de dependencia entre una variable dependiente $Y$ y una variable independiente $X$.

Nos interesan las relaciones de la forma $y = f(x) + u$, donde $u$ es una variable aleatoria a la que llamamos perturbación.
En el caso de la regresión lineal simple, el modelo será de la forma $$y = \beta_0 + \beta_1x + u$$
con $\beta_0$ y $\beta_1$ parámetros.
Llamamos intercepto a $\beta_0$ y pendiente a $\beta_1$.

\section{Modelo e hipótesis}
Sea $X$ una variable aleatoria cuantitativa, $Y$ una variable aleatoria continua y $(x_1, y_1), (x_2, y_2), \dots (x_n, y_n)$ un conjunto de datos.
Entonces el modelo de regresión lineal simple es
$$y_i = \beta_0 + \beta_1x_i + u_i, \quad i = 1, \dots n$$

\subsection*{Hipótesis del modelo}
\begin{enumerate}
    \item $E(u_i) = 0, \quad \forall i = 1, \dots n$.
    \item $Var(u_i) = \sigma^2, \quad \forall i = 1, \dots n$ (homocedasticidad)
    \item $u_i \sim N(0, \sigma^2), \quad \forall i = 1, \dots n$ (normalidad)
    \item $E(u_i u_j) = 0, \quad \forall i \neq j$ (independencia)
\end{enumerate}

\begin{note}
    En realidad, la cuarta hipótesis es de incorrelación ($Cov(u_i, u_j) = 0$).
    $$Cov(u_i, u_j) = E(u_i u_j) - E(u_i)E(u_j) = E(u_i u_j)$$
    Sin embargo, bajo normalidad la incorrelación y la independencia son equivalentes.
\end{note}

Podemos escribir las mismas hipótesis en términos de $y_i, \quad \forall i = 1, \dots n$.
\begin{enumerate}
    \item $E(y_i | x_i) = E(\beta_0 + \beta_1x_i + u_i) = \beta_0 + \beta_1x_i, \quad \forall i = 1, \dots n$ (linealidad)
    \item $Var(y_i | x_i) = Var(\beta_0 + \beta_1x_i + u_i) = \sigma^2, \quad \forall i = 1, \dots n$ (homocedasticidad)
    \item $y_i | x_i \sim N(\beta_0 + \beta_1x_i, \sigma^2), \quad \forall i = 1, \dots n$ (normalidad)
    \item $Cov(y_i, y_j) = 0, \quad \forall i \neq j$ (independencia)
\end{enumerate}

Podemos dar un significado real a $\beta_0$ y $\beta_1$:
\begin{itemize}
    \item $\beta_0$ es el valor medio de la variable $Y$ cuando $x_i$ toma el valor 0.
          $$E(y_i | x_i = 0) = \beta_0, \quad i = 1, \dots n$$
    \item $\beta_1$ es la variación media que experimenta la variable $Y$ cuando $x_i$ aumenta en una unidad.
          $$E(y_i | x_i+1) - E(y_i | x_i) = \beta_1, \quad i = 1, \dots n$$
\end{itemize}

\section{Estimación de los parámetros}
Queremos estimar $\beta_0$, $\beta_1$ y $\sigma^2$.
Con los estimadores $\hat{\beta_0}$ y $\hat{\beta_1}$ podemos estimar $$\hat{E(y_i | x_i)} = \hat{\beta_0} + \hat{\beta_1}x_i, \quad i = 1, \dots n$$

\subsection*{Método de máxima verosimilitud}
$y_i | x_i \sim N(\beta_0 + \beta_1x_i, \sigma^2), \quad i = 1, \dots, n$, así que podemos encontrar estimadores de máxima verosimilitud para los parámetros y para $\sigma^2$.

Usando el método de máxima verosimilitud llegamos las ecuaciones normales de la regresión:
$$\begin{cases}
        \frac{\partial \log{L}}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i) = 0 \\
        \frac{\partial \log(L)}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i=1}^n x_i(y_i - \beta_0 - \beta_1x_i) = 0
    \end{cases}$$

\begin{notation}
    $\hat{y_i} = \hat{E(y_i | x_i)} = \hat{\beta_0} + \hat{\beta_1}x_i$
\end{notation}

Si definimos el error o residuo como $e_i = y_i - \hat{y_i} = y_i - \hat{\beta_0} - \hat{\beta_1}x_i$, podemos escribir las ecuaciones normales de regresión de la siguiente forma:
$$\begin{cases}
        \sum_{i=1}^n e_i = 0 \\
        \sum_{i=1}^n x_ie_i = 0
    \end{cases}$$

Resolviendo este sistema, obtenemos los estimadores:
\begin{align*}
    \hat{\beta_1}  & = \frac{s_{XY}}{s_X^2}                                                                                 \\
    \hat{\beta_0}  & = \bar{y} - \frac{s_{XY}}{s_X^2}\bar{x}                                                                \\
    \hat{\sigma^2} & = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2 = \frac{1}{n} \sum_{i=1}^n e_i^2
\end{align*}

La ecuación de la recta resultante es:
$$\hat{y_i} = \bar{y} + \frac{s_{XY}}{s_X^2}(x_i - \bar{x})$$

\subsection*{Estimación por mínimos cuadrados}
Queremos minimizar la suma de los cuadrados de los errores $\sum_{i=1}^n e_i^2$, donde $e_i = y_i - \hat{y_i}$.
Para ello minimizamos la función $M(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i)^2$.
$$\begin{cases}
        \frac{\partial M}{\partial \beta_0}(\beta_0, \beta_1) = -2\sum(y_i - \beta_0 - \beta_1x_i) = 0 \\
        \frac{\partial M}{\partial \beta_1}(\beta_0, \beta_1) = -2\sum x_i(y_i - \beta_0 - \beta_1x_i) = 0
    \end{cases}$$

Simplificando obtenemos las ecuaciones normales de la regresión, como antes.
Así que los estimadores de $\beta_0$ y $\beta_1$ por máxima verosimilitud coinciden con los estimadores por mínimos cuadrados.

\subsection*{Estimación de la varianza}
Partiendo del estimador $\bar{\sigma^2} = \frac{1}{n} \sum_{i=1}^n e_i^2$ obtenido previamente, podemos llegar a una expresión equivalente:
$$\hat{\sigma^2} = s_Y^2 - \frac{s_{XY}^2}{s_X^2}$$

Veamos si este estimador es insesgado calculando su esperanza.
$$E(\hat{\sigma^2}) = E(\frac{\sum_{i=1}^n e_i^2}{n}) = \frac{1}{n}E(\sum_{i=1}^n e_i^2) = \frac{1}{n}\sigma^2(n-2)$$

\begin{note}
    $\frac{\sum_{i=1}^n e_i^2}{\sigma^2} \sim \chi^2_{n-2}, \quad E(\frac{\sum_{i=1}^n e_i^2}{\sigma^2}) = n-2$
\end{note}

Observamos que este estimador no es insesgado. Consideramos entonces:
$$s_R^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2$$
Este sí es un estimador insesgado de $\sigma^2$ y le llamamos varianza residual.
Tenemos la relación $s_R^2 = \frac{n}{n-2} \hat{\sigma^2}$.

\section{Propiedades de los estimadores}
Podemos escribir $\hat{\beta_1}$ de la forma:
$$\hat{\beta_1} = \sum_{i=1}^n w_iy_i, \quad w_i = \frac{x_i - \bar{x}}{ns_X^2}$$
Por las hipótesis del modelo, $y_i$ son normales e independientes, luego $\hat{\beta_1} \sim N$.
Podemos calcular:

\begin{itemize}
    \item $E(\hat{\beta_1}) = \beta_1$ (estimador insesgado)
    \item $V(\hat{\beta_1}) = \frac{\sigma^2}{ns_X^2}$
\end{itemize}
Por tanto, $\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{ns_X^2})$.

De forma análoga, podemos escribir:
$$\hat{\beta_0} = \sum_{i=1}^n (\frac{1}{n} - \bar{x}w_i)$$
Como las $y_i$ son normales e independientes, $\hat{\beta_0} \sim N$.
Calculamos:

\begin{itemize}
    \item $E(\hat{\beta_0}) = \beta_0$ (estimador insesgado)
    \item $V(\hat{\beta_0}) = \frac{\sigma^2}{n} (1 + \frac{\bar{x}^2}{s_X^2})$
\end{itemize}
Por tanto, $\hat{\beta_0} \sim N(\beta_0, \frac{\sigma^2}{n} (1 + \frac{\bar{x}^2}{s_X^2}))$.

En cuanto a $s_R^2$, sabemos que $\frac{1}{\sigma^2} \sum_{i_1}^n e_i^2 \sim \chi^2_{n-2}$.
Obtenemos que:
\begin{itemize}
    \item $E(s_R^2) = \sigma^2$
    \item $V(s_R^2) = \frac{2}{n-2} (\sigma^2)^2$
\end{itemize}

\section{Intervalos de confianza para los parámetros}
\subsection*{Intervalos de confianza para $\beta_1$}
\subsubsection*{Caso 1: $\sigma^2$ conocida}
Sabemos que $\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{ns_X^2})$. Entonces:
$$\frac{\hat{\beta_1} - \beta_1}{\frac{\sigma}{\sqrt{ns_X^2}}} \sim N(0, 1)$$
Por tanto, el intervalo de confianza para $\beta_1$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\beta_1) = \left( \hat{\beta_1} - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{ns_X^2}}, \hat{\beta_1} + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{ns_X^2}} \right)$$
donde $z_{1-\frac{\alpha}{2}}$ es el percentil de orden $(1 - \frac{\alpha}{2}) 100\%$ de una variable aleatoria $Z \sim N(0, 1)$.

\subsubsection*{Caso 2: $\sigma^2$ desconocida}
$$\begin{cases}
        \frac{\hat{\beta_1} - \beta_1}{\frac{\sigma}{\sqrt{ns_X^2}}} \sim N(0, 1) \\
        \frac{\sum_{i=1}^n e_i^2}{\sigma^2} = \frac{(n-2)s_R^2}{\sigma^2} \sim \chi^2_{n-2}
    \end{cases} \Rightarrow
    \frac{\frac{\hat{\beta_1} - \beta_1}{\frac{\sigma}{\sqrt{ns_X^2}}}}{\sqrt{\frac{(n-2)s_R^2}{\sigma^2} \frac{1}{n-2}}} = \frac{\hat{\beta_1} - \beta_1}{\frac{s_R}{\sqrt{ns_X^2}}} \sim t_{n-2}$$
Luego el intervalo de confianza para $\beta_1$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\beta_1) = \left( \hat{\beta_1} - t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{ns_X^2}}, \hat{\beta_1} + t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{ns_X^2}} \right)$$
donde $s_R = +\sqrt{s_R^2}$ y $t_{n-2, 1-\frac{\alpha}{2}}$ es el percentil de orden $(1 - \frac{\alpha}{2}) 100\%$ de una variable aleatoria $T \sim t_{n-2}$.

\subsection*{Intervalos de confianza para $\beta_0$}
\subsubsection*{Caso 1: $\sigma^2$ conocida}
Sabemos que $\hat{\beta_0} \sim N\left( \beta_0, \frac{\sigma^2}{n} \left( 1 + \frac{\bar{x}}{s_X^2} \right) \right)$.
Entonces, el intervalo de confianza para $\beta_0$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\beta_0) = \left( \hat{\beta_0} - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \sqrt{1 + \frac{\bar{x}^2}{s_X^2}}, \hat{\beta_0} + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \sqrt{1 + \frac{\bar{x}^2}{s_X^2}} \right)$$
donde $z_{1-\frac{\alpha}{2}}$ es el percentil de orden $(1 - \frac{\alpha}{2}) 100\%$ de una variable aleatoria $Z \sim N(0, 1)$.

\subsubsection*{Caso 2: $\sigma^2$ desconocida}
Razonando de forma análoga al caso de $\beta_1$, tenemos que:
$$\frac{\hat{\beta_0} - \beta_0}{\frac{s_R}{n} \sqrt{1 + \frac{\bar{x}^2}{s_X^2}}}$$
Por tanto, el intervalo de confianza para $\beta_0$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\beta_0) = \left( \hat{\beta_0} - t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{n}} \sqrt{1 + \frac{\bar{x}^2}{s_X^2}}, \hat{\beta_0} + t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{n}} \sqrt{1 + \frac{\bar{x}^2}{s_X^2}} \right)$$
donde $t_{n-2, 1-\frac{\alpha}{2}}$ es el percentil de orden $(1 - \frac{\alpha}{2}) 100\%$ de una variable aleatoria $T \sim t_{n-2}$.

\subsection*{Intervalos de confianza para $\sigma^2$}
Sabemos que $\frac{\sum_{i=1}^n e_i^2}{\sigma^2} = \frac{(n-2)s_R^2}{\sigma^2} \sim \chi^2_{n-2}$.
Queremos que $P(a < \sigma^2 < b) = 1-\alpha$.
\begin{align*}
    P(a < \sigma^2 < b) & = P\left( \frac{1}{b} < \frac{1}{\sigma^2} < \frac{1}{a} \right) =                          \\
                        & = P\left( \frac{(n-2)s_R^2}{b} < \frac{(n-2)s_R^2}{\sigma^2} < \frac{(n-2)s_R^2}{a} \right)
\end{align*}
Luego:
$$\begin{cases}
        \frac{(n-2)s_R^2}{b} = \chi^2_{n-2, \frac{\alpha}{2}} \Rightarrow b = \frac{(n-2)s_R^2}{\chi^2_{n-2, \frac{\alpha}{2}}} \\
        \frac{(n-2)s_R^2}{a} = \chi^2_{n-2, 1 - \frac{\alpha}{2}} \Rightarrow a = \frac{(n-2)s_R^2}{\chi^2_{n-2, 1 - \frac{\alpha}{2}}}
    \end{cases}$$
Por tanto, el intervalo de confianza para $\sigma^2$ a nivel de significación $\alpha$ tiene por extremos $a$ y $b$, es decir:
$$IC_{1-\alpha}(\sigma^2) = (a, b)$$

\section{Contraste de la regresión}
Consideramos el siguiente contraste de hipótesis:
$$\begin{cases}
        H_0 : \beta_1 = 0 \Leftrightarrow E(y|x) = \beta_0 \\
        H_1 : \beta_1 \neq 0 \Leftrightarrow E(y|x) = \beta_0 + \beta_1x
    \end{cases}$$
Fijamos el nivel de significación $\alpha$.
Podemos resolverlo de cuatro formas distintas.

\subsection*{Intervalos de confianza}
Sea $IC_{1-\alpha}(\beta_1)$ el intervalo de confianza para $\beta_1$ a nivel de significación $\alpha$.
Entonces:
\begin{itemize}
    \item Aceptamos $H_0$ a nivel de significación $\alpha$ si $0 \in IC_{1-\alpha}(\beta_1)$.
    \item Rechazamos $H_0$ a nivel de significación $\alpha$ en caso contrario.
\end{itemize}

\subsection*{Estadístico $T$}
Sabemos que $\frac{\hat{\beta_1} - \beta_1}{\frac{s_R}{\sqrt{ns_X^2}}} \sim t_{n-2}$.
Entonces $T = \frac{\hat{\beta_1}}{\frac{s_R}{\sqrt{ns_X^2}}} \sim t_{n-2}$ si $H_0$ es cierto.

Tomamos un $t_{exp}$.
\begin{itemize}
    \item Si $t_{exp} \in (-t_{n-2, 1-\frac{\alpha}{2}}, t_{n-2, 1-\frac{\alpha}{2}})$, o equivalentemente $|t_{exp}| \leq t_{n-2, 1-\frac{\alpha}{2}}$, aceptamos $H_0$ a nivel de significación $\alpha$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

\subsection*{Valor $p$}
Sea $p$ el valor $p$ o $p$-valor de la distribución. Entonces:
\begin{itemize}
    \item Si $p \geq \alpha$, aceptamos $H_0$ a nivel de significación $\alpha$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

\subsection*{Tabla ANOVA}
Partimos de que podemos escribir:
$$\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2 + \sum_{i=1}^n (\hat{y_i} - \bar{y})^2$$
Definimos:
\begin{itemize}
    \item Variabilidad total:
          $$VT = \sum_{i=1}^n (y_i - \bar{y})^2 = ns_Y^2$$
    \item Variabilidad no explicada:
          $$VNE = \sum_{i=1}^n (y_i - \hat{y_i})^2 = \sum_{i=1}^n e_i^2 = (n-2)s_R^2 = n\hat{\sigma^2} = n(s_Y^2 - \hat{\beta_1}^2s_X^2)$$
    \item Variabilidad explicada:
          $$VE = \sum_{i=1}^n (\hat{y_i} - \bar{y})^2 = VT - VNE = n\hat{\beta_1}^2 s_X^2$$
\end{itemize}
Observamos que:
$$\frac{VNE}{\sigma^2} = \frac{\sum_{i=1}^n e_i^2}{\sigma^2} \sim \chi^2_{n-2}$$
Además, como $\frac{\hat{\beta_1} - \beta_1}{\frac{\sigma}{\sqrt{ns_X^2}}} \sim N(0,1)$, entonces:
$$\frac{(\hat{\beta_1} - \beta_1)^2}{\frac{\sigma^2}{ns_X^2}} \sim \chi^2_1$$
Luego $\frac{\hat{\beta_1}^2}{\frac{\sigma^2}{ns_X^2}} = \frac{n\hat{\beta_1}^2s_X^2}{\sigma^2} = \frac{VE}{\sigma^2} \sim \chi^2_1$ si $H_0$ es cierta.

Consideramos ahora $F = \frac{\frac{VE}{\sigma^2} / 1}{\frac{VNE}{\sigma^2} / (n-2)} = \frac{VE}{s_R^2}$.
Observamos que $F \sim F_{1, n-2}$ si $H_0$ es cierta.

Tomamos un $F_{exp}$.
\begin{itemize}
    \item Aceptamos $H_0$ a nivel de significación $\alpha$ si $F_{exp} \leq F_{1, n-2, 1-\alpha}$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

La tabla ANOVA es de la forma:
\begin{center}
    \begin{tabular}{ c | c | c | c }
        Fuentes & Suma de cuadrados                   & Grados de libertad & Cocientes         \\
        \hline
        $VE$    & $\sum_{i=1}^n(\hat{y_i}-\bar{y})^2$ & 1                  & $\frac{VE}{1}$    \\
        $VNE$   & $\sum_{i=1}^n(y_i - \hat{y_i})^2$   & $n-2$              & $\frac{VNE}{n-2}$ \\
        $VT$    & $\sum_{i=1}^n(y_i - \bar{y})^2$     & $n-1$
    \end{tabular}
\end{center}
También se incluyen columnas para $F_{exp}$ y $p$-valor.

\begin{remark}
    Existe la siguiente relación entre $t_{exp}$ y $F_exp$:
    $$t_{exp}^2 = F_{exp}$$
\end{remark}

\section{Evaluación del ajuste}
Existen dos coeficientes para evaluar el ajuste del modelo: el coeficiente de correlación lineal y el coeficiente de determinación.

\subsection*{Coeficiente de correlación lineal}
El coeficiente de correlación lineal se define como:
$$r = \frac{s_{XY}}{s_X s_Y}, \quad -1 \leq r \leq 1$$
\begin{itemize}
    \item Si $r = 1$, se tiene dependencia lineal exacta positiva.
    \item Si $r = -1$, se tiene dependencia lineal exacta negativa.
    \item Si $r = 0$, las variables están incorreladas linealmente.
\end{itemize}
Se dice que el ajuste es bueno si $|r|$ es cercano a 1.
Si por el contrario $r$ se aproxima a 0, entonces las variables no tienen relación lineal.

\subsection*{Coeficiente de determinación}
El coeficiente de determinación se define como:
$$R^2 = \frac{VE}{VT}, \quad 0 \leq R^2 \leq 1$$
\begin{itemize}
    \item Si $R^2 = 1$ entonces $VE = VT$ luego $VNE = \sum_{i=1}^n (y_i - \hat{y_i})^2 = \sum_{i=1}^n e_i^2 = 0$.
          Por tanto, $e_i = 0$ para todo $i = 1, \dots n$, así que el ajuste lineal es exacto.
    \item Si $R^2 = 0$ entonces $VE = 0$, luego $VT = VNE$.
          Así que el ajuste lineal es pésimo.
\end{itemize}

\begin{theorem}
    El coeficiente de determinación coincide con el coeficiente de correlación lineal al cuadrado.
    Es decir, $$r^2 = R^2$$

    \begin{proof}
        $$R^2 = \frac{VE}{VT} = \frac{n\hat{\beta_1}^2s_X^2}{ns_Y^2} = \frac{\left(\frac{s_{XY}}{s_X^2}\right)^2 s_X^2}{s_Y^2} = \frac{s_{XY}^2}{s_X^2 s_Y^2} = r^2$$
    \end{proof}
\end{theorem}

\section{Predicción}

\subsection*{Estimación de las medias condicionadas}
Llamamos $m_0 = E(y | x=x_0) = \beta_0 + \beta_1x_0$.
Observamos que $m_0$ es un parámetro que podemos estimar de la forma:
$$\hat{m_0} = \hat{E(y | x=x_0)} = \hat{\beta_0} + \hat{\beta_1}x_0$$

\begin{theorem}
    $$\hat{m_0} \sim N\left( m_0, \frac{\sigma^2}{n} \left(1+\frac{(x_0-\bar{x})^2}{s_X^2}\right) \right)$$
\end{theorem}

\subsubsection*{Intervalos de confianza para $m_0$}
Podemos calcular los intervalos de confianza para $m_0$ con nivel de confianza de $100(1-\alpha)\%$.

Si $\sigma^2$ es conocida,
$$\left( \hat{m_0} - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \sqrt{1 + \frac{(x_0-\bar{x})^2}{s_X^2}}, \hat{m_0} + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \sqrt{1 + \frac{(x_0-\bar{x})^2}{s_X^2}} \right)$$

Si $\sigma^2$ es desconocida,
$$\left( \hat{m_0} - t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{n}} \sqrt{1 + \frac{(x_0-\bar{x})^2}{s_X^2}}, \hat{m_0} + t_{n-2, 1-\frac{\alpha}{2}} \frac{s_R}{\sqrt{n}} \sqrt{1 + \frac{(x_0-\bar{x})^2}{s_X^2}} \right)$$

\subsection*{Predicción de una observación futura}
Dado un conjunto de datos $(x_1, y_1), \dots, (x_n, y_n)$ y dado $x_0$ queremos predecir:
$$y_0 = \beta_0 + \beta_1 x_0 + u_0$$
donde $u_0$ es independiente a $u_1, \dots, u_n$ con $u_0 \sim N(0, \sigma^2)$.
Observamos que $y_0$ es una variable aleatoria, con estimador $\hat{y_0} = \hat{\beta_0} + \hat{\beta_1}x_0$.
La estimación puntual es:
$$\hat{y_0} = \hat{\beta_0} + \hat{\beta_1}x_0 = \hat{m_0}$$

Consideramos el error:
$$e_0 = y_0 - \hat{y_0} = \beta_0 + \beta_1x_0 + u_0 - (\hat{\beta_0} + \hat{\beta_1}x_0)$$
que también es una variable aleatoria.

\begin{theorem}
    $$e_0 \sim \left( 0, \sigma^2 \left(1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{ns_X^2} \right) \right)$$
\end{theorem}

\subsubsection*{Intervalos de pronóstico para $y_0$}
Podemos calcular los intervalos de pronóstico $IP_{1-\alpha}(y_0)$ para $y_0$ con contenido probabilístico $1-\alpha$.

Si $\sigma^2$ es conocida,
$$\left( \hat{y_0} - z_{1-\frac{\alpha}{2}} \sigma \sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{ns_X^2}}, \hat{y_0} + z_{1-\frac{\alpha}{2}} \sigma \sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{ns_X^2}} \right)$$

Si $\sigma^2$ es desconocida,
$$\left( \hat{y_0} - t_{n-2, 1-\frac{\alpha}{2}} s_R \sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{ns_X^2}}, \hat{y_0} + t_{n-2, 1-\frac{\alpha}{2}} s_R \sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{ns_X^2}} \right)$$

\section{Análisis de residuos y observaciones atípicas e influyentes}
\subsection*{Residuos}
El residuo de un dato es la diferencia entre su valor y la predicción mediante el modelo.
$$e_i = y_i - \hat{y_i}, \quad \forall i = 1, \dots n$$
El análisis de los residuos puede darnos información sobre el ajuste del modelo.

\subsection*{Observaciones atípicas}
Una observación atípica es un valor que es numéricamente distinto al resto de los datos.
Visualmente, es un dato que se sale del patrón.
Las observaciones atípicas pueden ser indicativas de errores de observación o errores en el modelo.
Un error de observación se debe a datos que pertenecen a una población diferente del resto de muestras, mientras que un error en el modelo puede ser debido a que la muestra depende una variable desconocida que no se han tenido en cuenta.

\subsection*{Observaciones influyentes}
Una observación influyente $(x_A, y_A)$ es una observación atípica cuya exclusión produce un cambio drástico en la recta de regresión.
Puede ser causada por un error de observación o por un modelo incorrecto.
Algunas posibles causas de que el modelo sea incorrecto son:
\begin{itemize}
    \item La relación entre $x$ e $y$ no es lineal cerca de $x_A$.
    \item La varianza aumenta mucho con $x$.
    \item Una variable desconocida ha tomado un valor distinto en $x_A$.
\end{itemize}

\subsection*{Puntos palanca}
Los puntos palanca son observaciones con un valor alto de $p_i$.
Estos tienen la capacidad de alterar en gran medida la recta de regresión.

\section{Transformaciones}
Cuando el diagrama de dispersión entre las dos variables o el de los residuos presenta indicios de incumplimiento de alguna hipótesis básica, entonces hay que abandonar el modelo inicial por uno menos simple o bien aplicar alguna transformación a los datos.

\chapter{Modelo de regresión lineal múltiple}
\section{Modelo e hipótesis}
Sean $X_1, \dots, X_n$ variables explicativas, $Y$ una variable aleatoria continua y $(x_{11}, x_{21}, \dots, x_{k1}, y_1), \dots, (x_{1n}, x_{2n}, \dots, x_{kn}, y_n)$ un conjunto de datos.
Entonces el modelo de regresión lineal múltiple es:
$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i, \quad i = 1, \dots, n$$

\subsection*{Hipótesis del modelo}
\begin{itemize}
    \item $E(u_i) = 0, \quad \forall i = 1, \dots n$.
    \item $Var(u_i) = \sigma^2, \quad \forall i = 1, \dots n$ (homocedasticidad)
    \item $u_i \sim N(0, \sigma^2), \quad \forall i = 1, \dots n$ (normalidad)
    \item $E(u_i u_j) = 0, \quad \forall i \neq j$ (independencia)
    \item $n > k+1$
    \item No existen relaciones lineales entre los $X_i$ (ausencia de multicolinealidad)
\end{itemize}
El modelo se puede escribir de forma matricial.
Definimos:
$$\vec{y} = \begin{pmatrix}
        y_1 \\ \vdots \\ y_n
    \end{pmatrix} \in \mathbb{R}^n, \quad
    \vec{\beta} = \begin{pmatrix}
        \beta_0 \\ \vdots \\ \beta_k
    \end{pmatrix} \in \mathbb{R}^{k+1}, \quad
    \vec{u} = \begin{pmatrix}
        u_1 \\ \vdots \\ u_n
    \end{pmatrix} \in \mathbb{R}^n$$
$$X = \begin{pmatrix}
        1      & x_{11} & x_{21} & \dots & x_{k1} \\
        1      & x_{12} & x_{22} & \dots & x_{k2} \\
        \vdots & \vdots & \vdots &       & \vdots \\
        1      & x_{1n} & x_{2n} & \dots & x_{kn}
    \end{pmatrix} \in \mathcal{M}_{n \times (k+1)}$$
Entonces el modelo es equivalente a:
$$\vec{y} = X\vec{\beta} + \vec{u}$$
Las hipótesis del modelo se pueden reescribir como:
\begin{itemize}
    \item $\vec{u} \sim N_n(\vec{0}, \sigma^2 I_n)$
    \item $n > k+1$
    \item Ausencia de multicolinealidad.
\end{itemize}
Podemos escribir las mismas hipótesis iniciales en términos de $y_i, \quad \forall 1, \dots, n$.
\begin{enumerate}
    \item $E(y_i | x_{1i}, \dots, x_{ki}) = \beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki}, \quad \forall i = 1, \dots n$ (linealidad)
    \item $Var(y_i | x_{1i}, \dots, x_{ki}) = \sigma^2, \quad \forall i = 1, \dots n$ (homocedasticidad)
    \item $y_i | x_{1i}, \dots, x_{ki} \sim N(\beta_0 + \beta_1x_{1i} + \dots + \beta_kx{ki}, \sigma^2), \quad \forall i = 1, \dots n$ (normalidad)
    \item $Cov(y_i, y_j) = 0, \quad \forall i \neq j$ (independencia)
    \item $n > k+1$
    \item No existen relaciones lineales entre los $X_i$ (ausencia de multicolinealidad)
\end{enumerate}
Escritas para el modelo en forma matricial quedan:
\begin{itemize}
    \item $\vec{y} \sim N_n(\vec{x}\vec{\beta}, \sigma^2 I_n)$
    \item $n > k+1$
    \item $rg(X) = k+1$
\end{itemize}

\section{Estimación de los parámetros}
Queremos estimar $\beta_0, \beta_1, \dots, \beta_k$, o análogamente $\hat{\vec{\beta}}$, y $\sigma^2$.
Con los estimadores $\hat{\beta_0}, \hat{\beta_1}, \dots, \hat{\beta_k}$ podemos estimar $$\hat{E(y_i | x_{1i}, \dots, x_{ki})} = \hat{\beta_0} + \hat{\beta_1}x_{1i} + \dots + \hat{\beta_k}x_{ki}, \quad i = 1, \dots n$$.

Procedemos mediante el método de mínimos cuadrados.
La función a minimizar es:
$$M(\beta_0, \dots, \beta_k) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_{1i} - \dots - \beta_kx_{ki})^2$$
Planteamos las ecuaciones:
$$\begin{cases}
        \frac{\partial M}{\partial \beta_0} (\beta_0, \dots, \beta_k) = -2\sum_{i=1}^n (y_i - \beta_0 - \beta_1x_{1i} - \dots - \beta_kx_{ki}) \\
        \frac{\partial M}{\partial \beta_k} (\beta_0, \dots, \beta_k) = -2\sum_{i=1}^n x_{ji}(y_i - \beta_0 - \beta_1x_{1i} - \dots - \beta_kx_{ki}), \quad k \geq 1
    \end{cases}$$
Estas son las ecuaciones normales de la regresión.

Resolviendo este sistema, llegamos a que $M$ alcanza el mínimo si:
$$X'\vec{y} = X'X\hat{\vec{\beta}}$$

Por la hipótesis de ausencia de multicolinealidad $X'X$ tiene inversa, así que podemos escribir:
$$\hat{\vec{\beta}} = (X'X)^{-1}X'\vec{y}$$

Para estimar la varianza $\sigma^2$ usaremos la varianza residual:
$$s_R^2 = \frac{e_i^2}{n-k-1}$$

\begin{note}
    $$\frac{\sum_{i=1}^n e_i^2}{\sigma^2} \sim \chi^2_{n-(k+1)}$$
\end{note}

\section{Propiedades de los estimadores}
Sobre el estimador $\hat{\vec{\beta}}$, sabemos que:
$$\begin{cases}
        \hat{\vec{\beta}} = (X'X)^{-1}X'\vec{y} \\
        \hat{y} \sim N_n(X\vec{\beta}, \sigma^2 I_n)
    \end{cases} \Rightarrow \hat{\vec{\beta}} \sim N_{k+1}(\vec{\beta}, \sigma^2(X'X)^{-1})$$

\begin{note}
    $$\begin{cases}
            \vec{x} \sim N_n(\vec{\mu}, \Sigma) \\
            \vec{y} = A\vec{x}
        \end{cases} \Rightarrow \vec{y} \sim N_k(A\vec{\mu}, A\Sigma A')$$
\end{note}

Tenemos además que:
$$Cov(\hat{\vec{\beta}}) = \sigma^2(X'X)^{-1} = \begin{pmatrix}
        V(\hat{\beta_0})                  & Cov(\hat{\beta_0}, \hat{\beta_1}) & \dots  & Cov(\hat{\beta_0}, \hat{\beta_k}) \\
        Cov(\hat{\beta_1}, \hat{\beta_0}) & V(\hat{\beta_1})                  & \dots  & Cov(\hat{\beta_1}, \hat{\beta_k}) \\
        \vdots                            & \vdots                            & \ddots & \vdots                            \\
        Cov(\hat{\beta_k}, \hat{\beta_0}) & Cov(\hat{\beta_k}, \hat{\beta_1}) & \dots  & V(\hat{\beta_k})
    \end{pmatrix}$$
Así que $\hat{\beta_j} \sim N(\beta_j, \sigma^2 q_{j+1, j+1})$ para $j = 0, \dots, k$, donde $q_{j+1, j+1}$ es el elemento $(j+1, j+1)$ de $(X'X)^{-1}$.
Equivalentemente, es el elemento $(j+1)$-ésimo de la diagonal principal de $(X'X)^{-1}$.

En cuanto a $s_R^2$,
$$\begin{cases}
        E(\frac{\sum_{i=1}^n e_i^2}{\sigma^2}) = n-k-1 \\
        Var(\frac{\sum_{i=1}^n e_i^2}{\sigma^2}) = 2(n-k-1)
    \end{cases} \Rightarrow \begin{cases}
        E(s_R^2) = \sigma^2 \\
        Var(s_R^2) = \frac{2(\sigma^2)^2}{n-k-1}
    \end{cases}$$

\section{Intervalos de confianza para los parámetros}
\subsection*{Intervalos de confianza para $\beta_j$, $j = 0, \dots, k$}
Supondremos $\sigma^2$ desconocida.

Sea $j \in \{0, \dots, k\}$, sabemos que $\hat{\beta_j} \sim N(\beta_j, \sigma^2 q_{j+1, j+1})$.
Así que:
$$\begin{cases}
        \frac{\hat{\beta_j} - \beta_j}{\sigma \sqrt{q_{j+1, j+1}}} \sim N(0, 1) \\
        \frac{(n-k-1)s_R^2}{\sigma^2} \sim \chi^2_{n-k-1}
    \end{cases} \Rightarrow \frac{\hat{\beta_j} - \beta_j}{s_R \sqrt{q_{j+1, j+1}}} \sim t_{n-k-1}$$

Luego el intervalo de confianza para $\beta_j$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\beta_j) = \left( \hat{\beta_j} - t_{n-k-1, 1-\frac{\alpha}{2}} s_R \sqrt{q_{j+1, j+1}}, \hat{\beta_j} + t_{n-k-1, 1-\frac{\alpha}{2}} s_R \sqrt{q_{j+1, j+1}} \right)$$

\subsection*{Intervalos de confianza para $\sigma^2$}
Sabemos que $\frac{(n-k-1)s_R^2}{\sigma^2} \sim \chi^2_{n-k-1}$.
Usando un desarrollo análogo al que hicimos para el modelo de regresión lineal simple, llegamos a que el intervalo de confianza para $\sigma^2$ a nivel de significación $\alpha$ es:
$$IC_{1-\alpha}(\sigma^2) = \left( \frac{(n-k-1)s_R^2}{\chi^2_{n-k-1, 1-\frac{\alpha}{2}}}, \frac{(n-k-1)s_R^2}{\chi^2_{n-k-1, \frac{\alpha}{2}}} \right)$$

\section{Contrastes de hipótesis para los coeficientes de regresión}
\subsection*{Contrastes de significación individuales}
Consideramos el contraste de hipótesis:
$$\begin{cases}
        H_0: \beta_j = 0 \\
        H_1: \beta_j \neq 0
    \end{cases} \quad j = 1, \dots, k$$
Este contraste indica si hay suficiente evidencia en la muestra para afirmar que $X_j$ tiene una influencia lineal significativa en el modelo.

Fijamos el nivel de significación $\alpha$.
Hay tres formas de resolver  el contraste.

\subsubsection*{Intervalos de confianza}
Sea $IC_{1-\alpha}(\beta_j)$ el intervalo de confianza para $\beta_j$ a nivel de significación $\alpha$.
Entonces:
\begin{itemize}
    \item Aceptamos $H_0$ a nivel de significación $\alpha$ si $0 \in IC_{1-\alpha}(\beta_j)$.
    \item Rechazamos $H_0$ a nivel de significación $\alpha$ en caso contrario.
\end{itemize}

\subsubsection*{Estadístico $T$}
Sabemos que $\frac{\hat{\beta_j} - \beta_j}{s_R \sqrt{q_{j+1, j+1}}} \sim t_{n-k-1}$.
Entonces $T = \frac{\hat{\beta_j}}{s_R \sqrt{q_{j+1, j+1}}} \sim t_{n-k-1}$ si $H_0$ es cierto.
Tomamos un $t_{exp}$.
\begin{itemize}
    \item Si $|t_{exp}| \leq t_{n-k-1, 1-\frac{\alpha}{2}}$, aceptamos $H_0$ a nivel de significación $\alpha$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

\subsubsection*{Valor $p$}
Sea $p$ el $p$-valor de la distribución. Entonces:
\begin{itemize}
    \item Si $p \geq \alpha$, aceptamos $H_0$ a nivel de significación $\alpha$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

\subsection*{Contraste de regresión}
Consideramos ahora el contraste de hipótesis:
$$\begin{cases}
        H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
        H_1: \exists i \in \{1, \dots, k\} : \beta_i \neq 0
    \end{cases}$$
Este contraste indica si hay suficiente evidencia en la muestra para afirmar que el modelo es globalmente o conjuntamente válido.

Recordamos que:
\begin{align*}
    \sum_{i=1}^n (y_i - \bar{y})^2 & = \sum_{i=1}^n (y_i - \hat{y_i})^2 + \sum_{i=1}^n (\hat{y_i} - \bar{y})^2 \\
    VT                             & = VNE + VE
\end{align*}

Observamos que:
$$\frac{VNE}{\sigma^2} = \frac{\sum_{i=1}^n e_i^2}{\sigma^2} \sim \chi^2_{n-k-1}$$
Se verifica que $\frac{VE}{\sigma^2} \sim \chi^2_k$ si $H_0$ es cierta.
Así que $\frac{VT}{\sigma^2} \sim \chi^2_{n-1}$ si $H_0$ es cierta.

Consideramos entonces el estadístico de contraste:
$$F = \frac{\frac{VE}{\sigma^2} / k}{\frac{VNE}{\sigma^2} / (n-k-1)} = \frac{(n-k-1)VE}{ks_R^2}$$
Observamos que $F \sim F_{k, n-k-1}$ si $H_0$ es cierta.

Tomamos un $F_{exp}$.
\begin{itemize}
    \item Aceptamos $H_0$ a nivel de significación $\alpha$ si $F_{exp} \leq F_{k, n-k-1, 1-\alpha}$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

La tabla ANOVA es de la forma:
\begin{center}
    \begin{tabular}{ c | c | c | c }
        Fuentes & Suma de cuadrados                   & Grados de libertad & Cocientes           \\
        \hline
        $VE$    & $\sum_{i=1}^n(\hat{y_i}-\bar{y})^2$ & $k$                & $\frac{VE}{k}$      \\
        $VNE$   & $\sum_{i=1}^n(y_i - \hat{y_i})^2$   & $n-k-1$            & $\frac{VNE}{n-k-1}$ \\
        $VT$    & $\sum_{i=1}^n(y_i - \bar{y})^2$     & $n-1$
    \end{tabular}
\end{center}
También se incluyen columnas para $F_{exp}$ y $p$-valor.

Veamos algunas expresiones para $VT$, $VNE$ y $VE$.
\begin{align*}
    VT  & = \sum (y_i - \bar{y})^2 = \sum y_i^2 + n\bar{y}^2 - 2\bar{y}\sum y_i = \sum y_i^2 + n\bar{y}^2 = \vec{y}'\vec{y} - n\vec{y}^2                     \\
    VNE & = \sum (y_i - \hat{y_i})^2 = (\vec{y} - \hat{\vec{y}})'(\vec{y} - \hat{\vec{y}}) = (\vec{y} - X\hat{\vec{\beta}})'(\vec{y} - X\hat{\vec{\beta}}) = \\
        & = \vec{y}'\vec{y} - \hat{\vec{\beta}}X'\vec{y} + (\hat{\vec{\beta}}'X'X - \vec{y}'X)\vec{\beta} = \vec{y}'\vec{y} - \hat{\vec{\beta}}X'\vec{y}     \\
    VE  & = VT - VNE = \vec{y}'\vec{y} - n\vec{y}^2 - \vec{y}'\vec{y} + \hat{\vec{\beta}}X'\vec{y} = \hat{\vec{\beta}}X'\vec{y} - n\vec{y}^2
\end{align*}

\begin{note}
    $$\hat{\beta}'X'X - \vec{y}'X = y'X(X'X)^{-1}X'X - y'X = y'X - y'X = 0$$
\end{note}

\subsection*{Interpretación de los contrastes sobre los coeficientes de regresión}
Los casos que se pueden presentar al realizar contrastes de hipótesis en un modelo de regresión son los siguientes:
\begin{center}
    \begin{tabular}{ c | c | c }
        Casos & Contraste conjunto & Contraste individual   \\
        \hline
        1     & Significativo      & Todos significativos   \\
        2     & Significativo      & Algunos significativos \\
        3     & Significativo      & Ninguno significativo  \\
        4     & No significativo   & Todos significativos   \\
        5     & No significativo   & Algunos significativos \\
        6     & No significativo   & Ninguno significativo  \\
    \end{tabular}
\end{center}
Significativo indica que se rechaza la hipótesis $H_0$ de que el parámetro o parámetros a los que se refiere la hipótesis sea 0.

Analicemos cada uno de los casos:
\begin{itemize}
    \item El caso 1 indica que todas las variables explicativas influyen.
    \item El caso 2 indica que solo influyen algunas variables explicativas, por lo que en principio se deberían eliminar las no significativas del modelo.
          Esto no debe hacerse mecánicamente, sino estudiando en profundidad cuál sería el modelo que se seleccionaría.
    \item El caso 3 corresponde al caso en que las $x$ son muy dependientes entre sí y, aunque conjuntamente infuyen, individualmente no son significativas.
          Es decir, se tiene multicolinealidad.
    \item El caso 4 es poco frecuente y es un tipo de multicolinealidad especial.
          Si dos variables influyen sobre $y$ pero en sentido contrario, su efecto conjunto puede ser no significativo aunque sus efectos individuales sí lo sean.
    \item El caso 5 es análogo al 4.
    \item En el caso 6 ninguna de las variables parece tener efecto sobre $y$ pero solo podremos decir que sus efectos no se detectan en la muestra considerada.
\end{itemize}

\subsection*{Constrastes de grupos de cocientes}
Consideramos ahora el contraste de hipótesis:
$$\begin{cases}
        H_0: \beta_1 = \beta_2 = \dots = \beta_i = 0 \\
        H_1: \exists j \in \{1, \dots, i\} : \beta_j \neq 0
    \end{cases}$$

Definimos:
\begin{itemize}
    \item $VE(k)$: variabilidad explicada por el modelo con $x_1, \dots, x_k$ como variables explicativas.
    \item $VE(k-i)$: variabilidad explicada por el modelo con todas las variables explicativas excepto $x_1, \dots, x_k$.
    \item $\Delta VE = VE(k) - VE(k-i)$: variabilidad explicada por $x_1, \dots, x_i$.
    \item $VNE(k)$: variabilidad no explicada por el modelo con $x_1, \dots, x_k$ como variables explicativas.
\end{itemize}

Consideramos el estadístico de contraste:
$$F = \frac{\Delta VE / i}{VNE(k) / (n-k-1)} = \frac{(n-k-1)\Delta VE}{iVNE(k)} = \frac{(n-k-1)\Delta VE}{is_R^2(n-k-1)} = \frac{\Delta VE}{is_R^2}$$
Observamos que $F \sim F_{i, n-k-1}$ si $H_0$ es cierta.

Tomamos un $F_{exp}$.
\begin{itemize}
    \item Si $F_{exp} \leq F_{i, n-k-1, 1-\alpha}$, aceptamos $H_0$ a nivel de significación $\alpha$.
    \item En caso contrario, rechazamos $H_0$ a nivel de significación $\alpha$.
\end{itemize}

Este constraste se puede utilizar para contrastes individuales:
$$\begin{cases}
        H_0: \beta_i = 0 \\
        H_1: \beta_i \neq 0
    \end{cases}$$
En este caso, $F_{exp} = t_{exp}^2$.

\section{Correlación en regresión múltiple}
\subsection*{Coeficiente de determinación}
Definimos el coeficiente de determinación como:
$$R^2 = \frac{VE}{VT} = \frac{VT - VNE}{VT} = 1 - \frac{VNE}{VT} = 1 - \frac{(n-k-1)s_R^2}{VT}, \quad 0 \leq R^2 \leq 1$$
Observamos que $R^2$ aumenta si el número de variables explicativas aumenta $k$, aunque las variables no sean significativas.

\subsection*{Coeficiente de determinación ajustado}
Definimos el coeficiente de determinación ajustado o corregido como:
$$\bar{R}^2 = 1 - \frac{VNE/(n-k-1)}{VT/(n-1)} = 1 - \frac{n-1}{n-k-1}\frac{(n-k-1)s_R^2}{VT} = 1 - (n-1)\frac{s_R^2}{VT}$$
Observamos que $\bar{R}^2$ aumenta si y solo si $s_R^2$ disminuye.

\begin{note}
    $\bar{R}^2$ puede ser negativo.
\end{note}

Veamos qué relación hay entre $R^2$ y $\bar{R}^2$.
$$R^2 = 1 - \frac{(n-k-1)s_R^2}{VT} \Rightarrow \frac{s_R^2}{VT} = \frac{1-R^2}{n-k-1}$$
Por tanto,
$$\bar{R}^2 = 1 - (n-1)\frac{s_R^2}{VT} = 1 - \frac{(n-1)(1-R^2)}{n-k-1} \Rightarrow 1 - \bar{R}^2 = \frac{n-1}{n-k-1}(1 - R^2)$$

Además,
$$n-k-1 \leq n-1 \Rightarrow \frac{n-1}{n-k-1}(1-R^2) \geq 1 - R^2 \Rightarrow 1-\bar{R}^2 \geq 1-R^2 \Rightarrow \bar{R}^2 \leq R^2$$
Luego $\bar{R}^2 \leq R^2 \leq 1$.

\section{Predicción}
\subsection*{Estimación de las medias condicionadas}
Queremos estimar:
$$m_0 = E(y|x_{10}, \dots, x_{k0}) = \beta_0 + \beta_1x_{10} + \dots + \beta_kx_{k0} = \vec{x_0}'\vec{\beta}$$
con $\vec{x_0} = (1, x_{10}, \dots, x_{k0})$.
Para ello usamos el estimador:
$$\hat{m_0} = \hat{E}(y|x_{10}, \dots, x_{k0}) = \hat{\beta_0} + \hat{\beta_1}x_{10} + \dots + \hat{\beta_k}x_{k0}$$
Como $\hat{\vec{\beta}} \sim N_{k+1}(\vec{\beta}, \sigma^2(X'X)^{-1})$,
$$\hat{m_0} \sim N(\vec{x_0}'\vec{\beta}, \vec{x_0}'V(\hat{\vec{\beta}})\vec{x_0}) \equiv N(m_0, \sigma^2\vec{x_0}'(X'X)^{-1}\vec{x_0})$$
Observamos  que $\hat{m_0}$ es un estimador insesgado.

Para obtener intervalos de confianza vemos que:
$$\frac{\hat{m_0}-m_0}{\sigma\sqrt{\vec{x_0}'(X'X)^{-1}\vec{x_0}}} \sim N(0, 1) \Rightarrow \frac{\hat{m_0}-m_0}{s_R\sqrt{\vec{x_0}'(X'X)^{-1}\vec{x_0}}} \sim t_{n-k-1}$$
Por tanto, el intervalo de confianza para $m_0$ a nivel de significación $\alpha$ es:
$$\left( \hat{m_0} - t_{n-k-1, 1-\frac{\alpha}{2}} s_R\sqrt{\vec{x_0}'(X'X)^{-1}\vec{x_0}}, \hat{m_0} + t_{n-k-1, 1-\frac{\alpha}{2}} s_R\sqrt{\vec{x_0}'(X'X)^{-1}\vec{x_0}} \right)$$

\subsection*{Predicción de una nueva observación}
Queremos predecir la variable aleatoria
$$y_0 = \beta_0 + \beta_1x_{10} + \dots + \beta_kx_{k0} + u_0 = \vec{x_0}'\vec{\beta} + u_0$$
Su estimación puntual es:
$$\hat{y_0} = \hat{\beta_0} + \hat{\beta_1}x_{10} + \dots + \hat{\beta_k}x_{k0} = \vec{x_0}'\hat{\vec{\beta}} = \hat{m_0}$$

Consideramos la variable aleatoria error:
$$e_0 = y_0 - \hat{y_0}$$
Sabemos que:
$$\begin{cases}
        y_0 \sim N(\vec{x_0}'\hat{\beta}, \sigma^2) \\
        \vec{y_0} \sim N(m_0, \sigma^2\vec{x_0}'(X'X)^{-1}\vec{x_0})
    \end{cases}$$
Como además $y_0$ e $\hat{y_0}$ son independientes, $e_0 \sim N$.
\begin{align*}
    E(e_0) & = E(y_0) - E(\hat{y_0}) = \vec{x_0}'\vec{\beta} - m_0 = 0                                                                 \\
    V(e_0) & = V(y_0) + V(\hat{y_0}) = \sigma^2 + \sigma^2\vec{x_0}'(X'X)^{-1}\vec{x_0} = \sigma^2 (1 + \vec{x_0}'(X'X)^{-1}\vec{x_0})
\end{align*}
Por tanto, $e_0 \sim N(0, \sigma^2 (1 + \vec{x_0}'(X'X)^{-1}\vec{x_0}))$.

Para obtener intervalos de pronóstico observamos que:
$$\frac{e_0}{\sigma\sqrt{1+\vec{x_0}'(X'X)^{-1}\vec{x_0}}} \sim N(0, 1) \Rightarrow \frac{e_0}{s_R\sqrt{1+\vec{x_0}'(X'X)^{-1}\vec{x_0}}} \sim t_{n-k-1}$$
Así que el intervalo de pronóstico para $y_0$ con contenido probabilístico $1-\alpha$ es:
$$IP_{1-\alpha}(y_0) = (\hat{y_0} - R, \; \hat{y_0} + R)$$
donde
$$R = t_{n-k-1, 1-\frac{\alpha}{2}} s_R\sqrt{1+\vec{x_0}'(X'X)^{-1}\vec{x_0}}$$

\section{Diagnosis y validación del modelo}
\subsection*{Multicolinealidad}
El primer problema que surge es la dependencia de las variables explicativas entre sí, es decir, la existencia de una o más combinaciones lineales entre las columnas de la matriz:
$$X = \begin{pmatrix}
        1      & x_{11} & \dots & x_{k1} \\
        1      & x_{12} & \dots & x_{k2} \\
        \vdots & \vdots &       & \vdots \\
        1      & x_{1n} & \dots & x_{kn}
    \end{pmatrix}, \quad rang(X) \leq k+1$$
Esto es equivalente a que $rang(X) < k+q$.

Cuando esto ocurre es difícil separar los efectos de cada variable explicativa y medir la contribución individual, con lo que los estimadores individuales serán inestables y con gran varianza.
A este problema se le denomina multicolinealidad y consiste en querer extraer de la muestra más información de la que contiene.

Existen dos tipos de multicolinealidad:
\begin{enumerate}
    \item \textbf{Multicolinealidad perfecta.}
          Se da cuando una de las variables explicativas es combinación lineal exacta de las demás.
          En este caso $rang(X) < k+1$ así que $\det(X'X) = 0$ y no se puede calcular $(X'X)^{-1}$.
          El sistema de ecuaciones que determina el vector $\hat{\beta}$ no tiene solución única.
    \item \textbf{Alta multicolinealidad.}
          Se da cuando alguna o todas las variables explicativas están altamente correlacionadas entre sí pero el coeficiente de correlación no llega a ser 1 ni -1.
          En este caso las columnas de la matriz $X$ tienen un alto grado de dependencia entre sí pero sí puede calcularse el vector $\hat{\beta}$.
          Sin embargo, presenta algunos problemas.
          \begin{itemize}
              \item Los estimadores $\hat{\beta_j}$ tendrán varianzas muy altas, lo que provocará mucha imprecisión en la estimación de los $\hat{\beta_j}$.
                    En consecuencia, los intervalos de confianza serán muy anchos.
              \item Los estimadores $\hat{\beta_j}$ serán muy dependientes entre sí, puesto que tendrán altas covarianzas y habrá poca información sobre lo que ocurre al variar una variable si las demás permanecen constantes.
          \end{itemize}
\end{enumerate}

\subsubsection*{Consecuencias de la multicolinealidad}
\begin{itemize}
    \item Los estimadores $\hat{\beta_j}$ serán muy sensibles a pequeñas variaciones en el tamaño muestral o la supresión de una variable aparentemente no significativa.
          A pesar de esto, la predicción no tiene por qué verse afectada ante la multicolinealidad, ni esta afecta al vector de residuos que está siempre bien definido.
    \item Los coeficientes de regresión pueden ser no significativos individualmente puesto que las varianzas de los $\hat{\beta_j}$ van a ser grandes, aunque el contraste global del modelo sea significativo.
    \item La multicolinealidad puede afectar mucho a algunos parámetros y nada a otros.
          Los parámetros que estén asociados a variables explicativas poco correlacionadas con el resto no se verán afectados y podrán estimarse con precisión.
\end{itemize}

\subsubsection*{Identificación de la multicolinealidad}
La identificación de variables correlacionadas se realiza de una de las siguientes formas:
\begin{enumerate}
    \item Examinando la matriz de correlaciones entre las variables explicativas $R$ y su inversa.
          La presencia de correlaciones altas entre variables explicativas es un indicio de multicolinealidad.
          Aun así, es posible que exista una relación perfecta entre una variable y el resto y, sin embargo, sus coeficientes de correlación sean bajos.

          Definimos la matriz de correlaciones como:
          $$R = \begin{pmatrix}
                  1      & r_{12} & r_{13} & \dots  & r_{1k} \\
                  r_{12} & 1      & r_{23} & \dots  & r_{2k} \\
                  r_{13} & r_{23} & 1      & \dots  & r_{3k} \\
                  \vdots & \vdots &        & \ddots & \vdots \\
                  r_{1k} & r_{2k} & r_{3k} & \dots  & 1
              \end{pmatrix}, \quad r_{ij} = \frac{s_{X_iX_j}}{s_{X_i}s_{X_j}}, \quad -1 \leq r_{ij} \leq 1$$
          Esta es una matriz de orden $k$, simétrica y con unos en la diagonal.

          La inversa de la matriz de correlaciones:
          $$R^{-1} = \begin{pmatrix}
                  \gamma_{11} & \gamma_{12} & \dots  & \gamma_{1k} \\
                  \gamma_{21} & \gamma_{22} & \dots  & \gamma_{2k} \\
                  \vdots      & \vdots      & \ddots & \vdots      \\
                  \gamma_{k1} & \gamma_{k2} & \dots  & \gamma_{kk}
              \end{pmatrix}$$
          tiene en cuenta la dependencia conjunta.
          Los elementos de su diagonal se denominan factores de incremente o de inflación de la variance y verifican:
          $$\gamma_{ii} = FIV(i) = \frac{1}{1-R^2_{i,r}}, \quad i = 1, \dots, k$$
          donde $R^2_{i,r}$ es el coeficiente de determinación de la regresión de la variable $X_i$ en función del resto de variables explicativas.
          Por tanto, si para algún $i$ se tiene que:
          $$\gamma_{ii} > 10 \Leftrightarrow \frac{1}{1-R^2_{i,r}} > 10 \Leftrightarrow 1 - R^2_{i,r} < 0.1 \Leftrightarrow R^2_{i,r} > 0.9$$
          es decir, la variable $X_i$ se explica como mínimo en un 90\% por el resto de variables explicativas.
          Luego estamos en una situación de alta multicolinealidad.

          $R^{-1}$ se calculará con poca precisión cuando $R$ sea casi singular.
    \item Examinando los autovalores de $X'X$ o de $R$.
          Las mejores medidas de singularidad de $X'X$ o de $R$ utilizan los autovalores de estas matrices.
          Un índice de singularidad que se utiliza en cálculo numérico es el índice de condicionamiento.

          Si $M$ es una matriz de orden $k$, simétrica y definida positiva, y $\lambda_1 < \lambda_2 < \dots < \lambda_k$ son sus autovalores, se define el índice de condicionamiento de $M$ como:
          $$cond(M) = \sqrt{\frac{\lambda_k}{\lambda_1}} \geq 1$$
          Es más conveniente calcular este índice para $R$ que para $X'X$, con el fin de evitar la influencia de las escalas de medida de los regresores.

          Para saber si existe o no multicolinealidad, calcularemos $cond(R)$ y:
          \begin{itemize}
              \item Si $cond(R) > 30$, se tiene alta multicolinealidad.
              \item Si $10 < cond(R) < 30$, se tiene multicolinealidad moderada.
              \item Si $cond(R) < 10$, se tiene ausencia de multicolinealidad.
          \end{itemize}
\end{enumerate}

\subsubsection*{Tratamiento de la multicolinealidad}
Cuando la recogida de datos se diseñe a priori, la multicolinealidad puede evitarse tomando las observaciones de manera que la matriz $X'X$ sea diagonal, lo que aumentará la precisión en la estimación.

La multicolinealidad es un problema de la muestra y, por tanto, no tiene solución simple ya que estamos pidiendo a los datos más información de la que contienen.
Las dos únicas soluciones son:
\begin{itemize}
    \item Eliminar regresores, reduciendo el número de parámetros a estimar.
    \item Incluir información externa a los datos.
\end{itemize}

\subsection*{Análisis de los residuos}
Los residuos se definen como:
$$e_i = y_i - \hat{y_i}, \quad i = 1, \dots, n$$
Consideramos el vector de residuos:
\begin{align*}
    \vec{e} & = \vec{y} - \hat{\vec{y}} = \vec{y} - X\hat{\vec{\beta}} = \vec{y} - X(X'X)^{-1}X'\vec{y} = (I - X(X'X)^{-1}X')\vec{y} = \\
            & = (I-H)\vec{y} = (I-H)(X\vec{\beta}+\vec{u}) = X\vec{\beta} + \vec{u} - HX\vec{\beta} - H\vec{u} =                       \\
            & = X\vec{\beta} + \vec{u} - X\vec{\beta} - H\vec{u} = \vec{u} - H\vec{u} = (I-H)\vec{u}
\end{align*}
donde $H = X(X'X)^{-1}X'$ es una matriz simétrica e idempotente.
Veamos esto último:
$$H^2 = X(X'X)^{-1}X'X(X'X)^{-1}X' = X(X'X)^{-1}X' = H$$

\begin{note}
    $$HX\vec{\beta} = X(X'X)^{-1}X'X\vec{\beta} = X\vec{\beta}$$
\end{note}

Como $\vec{u} \sim N_n(\vec{0}, \sigma^2I_n)$, entonces $\vec{e} \sim N_n(\vec{0}, (I-H)'\sigma^2I(I-H))$.
Obtengamos una expresión más simplificada usando las propiedades de $H$:
$$(I-H)'\sigma^2I(I-H) = \sigma^2(I-H)^2 = \sigma^2(I-H)$$
Por tanto, $\vec{e} \sim N(\vec{0}, \sigma^2(I-H))$.
Además, podemos ver que $e_i \sim N(0, \sigma^2(1-h_{ii}))$, donde $h_{ii}$ es el elemento $(i,i)$ de la matriz $H$.
Este resultado es válido para la regresión lineal simple.

Se definen los residuos estandarizados como:
$$r_i = \frac{e_i}{s_R\sqrt{1-h_{ii}}} \sim t_{n-k-1}$$

\begin{note}
    $$\begin{cases}
            \frac{e_i}{\sigma\sqrt{1-h_{ii}}} \sim N(0, 1) \\
            \frac{(n-k-1)s_R^2}{\sigma^2} = \frac{\sum e_i^2}{\sigma^2} \sim \chi^2_{n-k-1}
        \end{cases} \Rightarrow
        \frac{\frac{e_i}{\sigma\sqrt{1-h_{ii}}}}{\sqrt{\frac{(n-k-1)s_R^2}{\sigma^2(n-k-1)}}} = \frac{e_i}{s_R\sqrt{1-h_{ii}}} \sim t_{n-k-1}$$
\end{note}

Se definen los residuos estudentizados como:
$$t_i = \frac{e_i}{s_R(i)\sqrt{1-h_{ii}}} \sim t_{n-k-2}$$
donde $s_R^2(i)$ es la varianza muestral de todos los datos excepto el $i$-ésimo.

\subsubsection*{Análisis gráfico de los residuos}
\begin{enumerate}
    \item \textbf{Histograma y gráfico probabilístico normal.}
          Sirve para detectar si hay normalidad y datos atípicos.
    \item \textbf{Gráfico de residuos frente a los valores predichos.}
          Sirve para comprobar si hay linealidad, homocedasticidad y datos atípicos.
          Se representan los residuos $t_i$ frente a los $\hat{y_i}$.
    \item \textbf{Gráficos de residuos frente a variables explicativas.}
          Detectan si hay linealidad, homocedasticidad y datos atípicos en cada variable.
          Se hacen $k$ gráficos, cada uno representando los residuos $t_i$ frente a cada variable $X_{ji}$, para $j = 1, \dots, k$.
    \item \textbf{Gráficos parciales de residuos.}
          Miden la influencia de cada $X_i$ quitando todas las demás variables.
          Se hacen $k$ gráficos con el siguiente procedimiento para cada $X_i$ con $i = 1, \dots, k$:
          \begin{enumerate}
              \item Ajustamos el modelo con todas las variables explicativas salvo $X_i$.
              \item Calculamos los errores del ajuste anterior $t_j^{(i)}$ y los representamos frente a $X_i$.
          \end{enumerate}
    \item \textbf{Gráfico de residuos frente a variables omitidas.}
          Sirve para comprobar si una variable omitida $X_{k+1}$ debería ser tenida en cuenta en el modelo.
          Se representan los residuos frente a $X_{k+1}$.
          Una estructura lineal en esta gráfica indica que hay que tener en cuenta esta variable.
\end{enumerate}

\subsubsection*{Observaciones atípicas e influyentes}
La observación $i$-ésima es atípica a nivel de significación $\alpha$ si $|t_i| > t_{n-k-2, 1-\frac{\alpha}{2}}$.

Una observación es influyente si se da alguno de estos casos:
\begin{itemize}
    \item Modifica el vector $\hat{\vec{\beta}}$ de parámetros estimado.
    \item Modifica el vector $\hat{\vec{y}}$ de predicciones.
    \item Hace que la observación del punto sea muy buena cuando este se incluye en el modelo y mala cuando se excluye.
\end{itemize}
En general son puntos palanca.

Definimos la distancia de Cook de la observación $i$-ésima como:
$$D(i) = \frac{(\hat{\vec{\beta}}-\hat{\vec{\beta}}_{(i)})'X'X(\hat{\vec{\beta}}-\hat{\vec{\beta}}_{(i)})}{(k+1)s_R^2}$$
donde $\hat{\vec{\beta}}_{(i)}$ es el vector de parámetros estimado sin la observación $i$-ésima.

\begin{note}
    Recordamos que:
    $$\begin{cases}
            \frac{(\hat{\vec{\beta}}-\vec{\beta})'X'X(\hat{\vec{\beta}}-\vec{\beta})}{\sigma^2} \sim \chi^2_{k+1} \\
            \frac{(n-k-1s_R^2)}{\sigma^2} = \frac{\sum e_i^2}{\sigma^2} \sim \chi^2_{n-k-1}
        \end{cases}$$
    Entonces:
    $$\frac{\frac{(\hat{\vec{\beta}}-\vec{\beta})'X'X(\hat{\vec{\beta}}-\vec{\beta})}{\sigma^2} / (k+1)}{\frac{(n-k-1s_R^2)}{\sigma^2} = \frac{\sum e_i^2}{\sigma^2} / (n-k-1)} = \frac{(\hat{\vec{\beta}}-\vec{\beta})'X'X(\hat{\vec{\beta}}-\vec{\beta})}{(k+1)s_R^2} \sim F_{k+1, n-k-1}$$
\end{note}

Usando esta distancia, podemos determinar que la observación $i$-ésima es influyente a nivel de significación $\alpha$ si:
$$D(i) > F_{k+1, n-k-1, 1-\alpha}$$

\begin{note}
    Una distancia $D(i) > 1$ suele indicar que la observación es influyente.
\end{note}

\section{Selección de modelos}
Distinguimos dos tipos de medidas para la bondad del modelo:
\begin{enumerate}
    \item Criterios basados en la bondas de ajuste:
          \begin{itemize}
              \item \textbf{Coeficiente de determinación.}
                    No sirve para comparar modelos en general, porque aquel que tenga más variables explicativas tiene un mayor $R^2$, incluso si no son significativas.
              \item \textbf{Coeficiente de determinación ajustado.}
                    Es mejor modelo el que tenga mayor $\bar{R}^2$.
              \item \textbf{Varianza residual.}
                    Es mejor modelo el que tenga menor $s_R^2$.
                    Es equivalente al anterior criterio por la relación que hay entre $\bar{R}^2$ y $s_R^2$.
          \end{itemize}
    \item Criterios basados en buscar buenas predicciones.
          \begin{itemize}
              \item \textbf{AIC (Akaike Information Criterion).}
                    Es mejor modelo el que tenga menor AIC.
              \item \textbf{BIC (Bayesian Information Criterion).}
                    Es mejor modelo el que tenga menor BIC.
          \end{itemize}
\end{enumerate}

Si dos modelos tienen una bondad similar, siempre es preferible el más simple.

\end{document}