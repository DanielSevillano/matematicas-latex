\chapter{Función característica}
\begin{definition}
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$.
    La función característica asociada a $X$ es:
    $$\varphi_X: \mathbb{R} \to \mathbb{C}$$
    $$\varphi_X(t) = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x)$$
\end{definition}

\begin{remark}
    Usando que $e^{ix} = \cos(x) + i\sin(x)$, podemos escribir:
    $$\varphi_X(t) = \int_\mathbb{R} \cos(tx)dF(x) + i \int_\mathbb{R} \sin(tx)dF(x)$$
\end{remark}

\begin{example}
    Sea $X \sim \delta(a)$, con $a \in \mathbb{R}$.
    Su función característica es:
    $$\varphi_X(t) = E(e^{itX}) = e^{ita} P(X=a) = e^{ita}$$
\end{example}

\begin{example}
    Sea $X \sim Bi(n, p)$, con $n \geq 0$ y $0 \leq p \leq 1$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \sum_{k=0}^n e^{itk} P(X=k) = \sum_{k=0}^n e^{itk} \binom{n}{k} p^k(1-p)^{n-k} = \\
                     & = \sum_{k=0}^n \binom{n}{k} (pe^{it})^k(1-p)^{n-k} = (pe^{it} + 1 - p)^n
    \end{align*}
\end{example}

\begin{example}
    Sea $X \sim Po(\lambda)$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \sum_{k=0}^n e^{itk} P(X=k) = \sum_{k=0}^n e^{itk}e^{-\lambda}\frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^n \frac{(\lambda e^{it})^k}{k!} = \\
                     & = e^{-\lambda}e^{\lambda e^{it}} = e^{\lambda(e^{it} - 1)}
    \end{align*}
\end{example}

\begin{remark}
    $$\varphi_{Bi}(x) = (pe^{it} + 1 - p)^n = \left( 1 + \frac{np(e^{it}-1)}{n} \right)^n \xrightarrow[n \to \infty]{np \to \lambda} e^{\lambda(e^{it} - 1)} = \varphi_{Po}(t)$$
\end{remark}

\begin{example}
    Sea $X \sim U([0, 1])$.
    Su función característica es:
    $$\varphi_X(t) = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x) = \int_\mathbb{R} e^{itx}f_X(x)dx = \int_0^1 e^{itx}dx = \frac{e^{it}-1}{it}$$
\end{example}

\begin{example}
    Sea $X \sim N(0, 1)$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \int_\mathbb{R} e^{itx}f_X(x)dx = \int_\mathbb{R} e^{itx} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-\frac{(x^2-2itx)}{2}}dx =                     \\
                     & = \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-\frac{t^2}{2}}e^{-\frac{x^2-t^2-2it}{2}}dx = \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} \int_\mathbb{R} e^{-\frac{(x-it)^2}{2}}dx = e^{-\frac{t^2}{2}}
    \end{align*}

    \begin{note}
        $\int_\mathbb{R} e^{-x^2}dx = \sqrt{\pi}$
    \end{note}
\end{example}

\section{Propiedades}
Veamos las propiedades más importantes de las funciones características.
Sea $\varphi$ la función característica de una variable aleatoria $X$.
Entonces:
\begin{enumerate}
    \item $\varphi(0) = 1$.
          $$E(e^{i0X}) = E(1) = 1$$
    \item $|\varphi(t)| \leq 1$, para todo $t \in \mathbb{R}$.
          $$|\varphi(t)| = |E(e^{itX})| \leq E|e^{itX}| = E|\cos(tx) + i\sin(tx)| = 1$$
    \item $\varphi(-t) = \overline{\varphi(t)}$.
          \begin{align*}
              \varphi(-t) & = E(e^{itX}) = E(\cos(-tx) + i\sin(-tx)) =                             \\
                          & = E(\cos(tx)) - iE(\sin(tx)) = \overline{E(\cos(tx)) + iE(\sin(tx))} = \\
                          & = \overline{E(\cos(tx)) + i\sin(tx)} = \overline{\varphi(t)}
          \end{align*}
    \item $\varphi$ es función definida positiva, es decir,
          $$\sum_{k,j=1}^n z_k\varphi(t_j-t_k)\overline{z_j} \geq 0, \quad \forall n \geq 1, \quad \forall z \in \mathbb{C}^n$$
\end{enumerate}

Además,
\begin{itemize}
    \item $\varphi \in \mathbb{R}^n$ $\Leftrightarrow$ $f$ es simétrica.
          $$\varphi(-t) = \overline{\varphi(t)} = \varphi(t)$$
    \item Sea $Y = a + bX$. Entonces:
          $$\varphi_Y(t) = E(e^{it(a+bX)}) = e^{ita} E(e^{itbX}) = e^{ita} \varphi_X(bt)$$
\end{itemize}

\begin{theorem}
    $\varphi$ es uniformemente continua en $\mathbb{R}$.
\end{theorem}

\section{Teorema de inversión}
\begin{theorem}[Teorema de inversión]
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$ y función característica $\varphi$.
    Sean $a, b \in \mathbb{R}$ con $a < b$, entonces:
    $$\frac{F(b)+F(b^-)}{2} - \frac{F(a)+F(a^-)}{2} = \lim\limits_{T \to \infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-itb}-e^{-ita}}{-it} \varphi(t)dt$$
\end{theorem}

\begin{corollary}
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$ y función característica $\varphi$.
    Sean $a, b \in C(F)$ con $a < b$, entonces:
    $$F(b) - F(a) = \lim\limits_{T \to \infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-itb}-e^{-ita}}{-it} \varphi(t)dt$$
\end{corollary}

\begin{theorem}[Unicidad]
    Sean $X_1$ y $X_2$ variables aleatorias, con funciones de distribución $F_1$ y $F_2$ y funciones características $\varphi_1$ y $\varphi_2$ respectivamente.
    Entonces:
    $$F_1 = F_2 \Leftrightarrow \varphi_1 = \varphi_2$$
\end{theorem}

\begin{theorem}
    Existe $k \in (0, \infty)$ tal que para todo $a > 0$ y toda medida de probabilidad $P_F$ se tiene que:
    $$P_F \left(\left[-\frac{1}{a}, \frac{1}{a}\right]^c\right) \leq \frac{k}{a} \int_0^a (1 - Re(\varphi_F(t)))dt$$
    donde $\varphi_F$ es la función característica asociada a $P_F$.
\end{theorem}

\begin{corollary}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias $X_n$ con $F_n$, $P_{F_n}$ y $\varphi_{F_n}$.
    Supongamos que:
    \begin{enumerate}
        \item Existe $\delta > 0$ tal que $\varphi_{F_n}(t) \xrightarrow[n \to \infty]{} \varphi(t)$ para todo $t \in [-\delta, \delta]$, siendo $\varphi$ una función.
        \item $\varphi$ es continua en 0.
    \end{enumerate}
    Entonces $\{X_n\}_{n \geq 1}$ es una sucesión ajustada.
    Es decir, $\{F_n\}$ forma una familia ajustada.
\end{corollary}

\begin{exercise}
    Sea $X$ con función de densidad:
    $$f(x) = \begin{cases}
            \frac{1}{a} \left(1 - \frac{|x|}{a}\right) & \text{si } |x| \leq a, \; a > 0 \\
            0                                          & \text{en otro caso}
        \end{cases}$$
    Calculemos su función característica $\varphi$.

    Como $f$ es simétrica, sabemos que $\varphi \in \mathbb{R}$.
    \begin{align*}
        \varphi(t) & = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x) = \int_\mathbb{R} e^{itx}f(x)dx =                                           \\
                   & = \int_{-a}^a \cos(tx)f(x)dx + \int_{-a}^a \sin(tx)f(x)dx = 2\int_0^a \cos(tx)\frac{1}{a}\left(1-\frac{x}{a}\right)dx = \\
                   & = \frac{2(1-\cos(at))}{a^2t^2} = \frac{\sin^2\left(\frac{at}{2}\right)}{\left(\frac{at}{2}\right)^2}
    \end{align*}

    En el último paso hemos usado que $\sin^2(\frac{\alpha}{2}) = \frac{1-\cos(\alpha)}{2}$.
\end{exercise}

\begin{exercise}
    Sea $X$ con función de densidad:
    $$f(x) = \frac{1}{2\beta}e^{-\frac{|x-\alpha|}{\beta}}, \quad \alpha \in \mathbb{R}, \; \beta > 0$$
    Calculemos su función característica $\varphi_X$.

    Para facilitar los cálculos, consideramos la variable estandarizada $Y = \frac{X-\alpha}{\beta}$ y calculamos $\varphi_Y$.
    Para ello, hallamos primero $F_Y$ y $f_Y$:
    \begin{align*}
        F_Y(y) & = P(Y \leq y) = P\left(\frac{X-\alpha}{\beta} \leq y\right) = P(X \leq \beta y + \alpha) = F_X(\beta y + \alpha) \\
        f_Y(y) & = F_Y'(y) = F_X'(\beta y + \alpha)\beta = f_X(\beta y + \alpha)\beta = \frac{1}{2}e^{-|y|}
    \end{align*}
    Observamos que $f_Y$ es simétrica, así que $\varphi_Y \in \mathbb{R}$.
    \begin{align*}
        \varphi_Y(t) & = \int_\mathbb{R} e^{ity}\frac{1}{2}e^{-|y|}dy = \int_\mathbb{R} (\cos(ty) + i\sin(ty))\frac{1}{2}e^{-|y|}dy = \\
                     & = \int_0^\infty \cos(ty)e^{-y}dy = \frac{1}{1+t^2}
    \end{align*}
    Por tanto:
    $$\varphi_X(t) = e^{it\alpha}\varphi_Y(\beta t) = \frac{e^{it\alpha}}{1+\beta^2t^2}$$

    También se puede ver que $Y = Y_1 - Y_2$, con $Y_i \sim Exp(1)$ independientes.
    De esta forma:
    \begin{align*}
        \varphi_Y(t) & = E(e^{itY}) = E(e^{it(Y_1-Y_2)}) = E(e^{itY_1})E(e^{-itY_2}) = \varphi_{Y_1}(t)\varphi_{Y_2}(-t) = \\
                     & = \varphi_{Y_1}(t) \overline{\varphi_{Y_2}(t)} = \frac{1}{1-it}\frac{1}{1+it} = \frac{1}{1+t^2}
    \end{align*}
\end{exercise}

\begin{theorem}[Teorema de continuidad de Lévy]
    Sea $\{X_n\}_{n \geq 1}$ y sean $\varphi_n$ las funciones de distribución asociadas.
    Supongamos que existe una función $\varphi$ tal que:
    \begin{enumerate}
        \item $\varphi_n(t) \to \varphi(t)$, $t \in \mathbb{R}$.
        \item $\varphi$ es continua en 0.
    \end{enumerate}
    Entonces $X_n \xrightarrow{d} X$, donde $X$ es la variable aleatoria con función característica $\varphi$.
\end{theorem}

\begin{theorem}
    Una sucesión $\{F_n\}_{n \geq 1}$ es ajustada si y solo si $$\lim\limits_{t \to 0} \left( \limsup\limits_{n \to \infty} Re(1 - \varphi_n(t))\right) = 0$$
\end{theorem}

\begin{remark}[Teorema central del límite de De Moivre]
    Sean $X_n \sim Bi(n, p)$.
    Sabemos que $E(X_n) = np$ y $V(X_n) = npq$, con $q = 1-p$.
    Consideramos:
    $$Z_n = \frac{X_n - np}{\sqrt{npq}} = \frac{X_n - np}{\sigma_n}$$
    Veamos que $Z_n \xrightarrow{d} Z$.

    Calculamos:
    \begin{align*}
        \varphi_{Z_n}(t) & = E(e^{itZ_n}) = E(e^{it\frac{X_n-np}{\sigma_n}}) = e^{-\frac{itnp}{\sigma_n}} E(e^{\frac{it}{\sigma_n}X_n}) = e^{-\frac{itnp}{\sigma_n}} \varphi_{X_n} \left(\frac{t}{\sigma_n}\right) = \\
                         & = e^{-\frac{itnp}{\sigma_n}} (pe^{i\frac{t}{\sigma_n}} + q)^n = (pe^{i\frac{t}{\sigma_n}q} + qe^{-i\frac{t}{\sigma_n}p})^n
    \end{align*}

    Se puede comprobar que:
    $$\lim\limits_{n \to \infty} \varphi_{Z_n}(t) = \lim\limits_{n \to \infty} (pe^{i\frac{t}{\sigma_n}q} + qe^{-i\frac{t}{\sigma_n}p})^n = e^{-\frac{t^2}{n}} = \varphi_Z(t)$$
    con $Z \sim N(0, 1)$.

    Por el teorema de continuidad de Lévy, $Z_n \xrightarrow{d} Z \sim N(0, 1)$.
\end{remark}

\section{Momentos}
Recordamos los momentos de una variable aleatoria $X$.
\begin{itemize}
    \item Momento de orden $n$: $E(X^n)$.
    \item Momento central de orden $n$: $E((X-E(X))^n)$.
    \item Momento absoluto de orden $n$: $E(|X|^n)$.
    \item Momento central absoluto de orden $n$: $E(|X-E(X)|^n)$.
\end{itemize}

\begin{proposition}
    Si $E(|X|^p) < \infty$ para algún $n \geq 1$, entonces:
    $$E(X^r), E(|X|^r) < \infty, \quad 0 < r \leq n$$
\end{proposition}

\begin{definition}
    El espacio $L^p$ es el conjunto de las variables $X$ tales que $E(|X|^r) < \infty$.
    $$L^r = \left\{X \text{ variable aleatoria } : \int |X|^rdF(x) < \infty\right\}$$

    $(L^p, \|.\|_p)$ es un espacio normado, con $\|X\|_p = (E(|X|^p))^{1/p}$, $X \in L^p$.
\end{definition}

\begin{theorem}
    Sea $X \in L^n$ para algún $n \geq 1$ y con función característica $\varphi$.
    Entonces existen las derivadas $\varphi^{(k)}$ con $k = 1, \dots, n$ y son uniformemente continuas.
    Además,
    $$\varphi^{(k)}(t) = i^k \int x^ke^{itx}dF(x)$$
    y $\varphi^{(k)}(0) = i^kE(X^k)$.
    Así que $\varphi$ se puede expresar como:
    $$\varphi(t) = 1 + \sum_{k=1}^n \frac{(it)^k}{k!}E(X^k) + O(t^n)$$
\end{theorem}

\begin{proposition}
    Sean $X$ e $Y$ variables aleatorias independientes con funciones de distribución $\varphi_X$ y $\varphi_Y$ respectivamente.
    Entonces la función característica de $S = X+Y$ es $\varphi_S(t) = \varphi_X(t)\varphi_Y(t)$.
\end{proposition}

En general, si $X_1, \dots, X_n$ son variables aleatorias independientes, la función característica de $S = X_1 + \dots + X_n$ es:
$$\varphi_S(t) = \prod_{i=1}^n \varphi_{X_i}(t)$$
Si además $X_1, \dots, X_n$ son igualmente distribuidas, entonces $\varphi_S(t) = \varphi_{X_k}(t)^n$ para cualquier $k \in \{1, \dots, n\}$.

\begin{exercise}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independeintes, cada una con función característica:
    $$\varphi_{X_n}(t) = \left(\frac{1 - (1-\alpha)it}{1-it}\right), \quad 0 \leq \alpha \leq 1$$

    \begin{enumerate}
        \item \textbf{Determinar la distribución de $X_n$.}
              \begin{align*}
                  \varphi_{X_n}(t) & = \left(\frac{1 - \alpha + \alpha - (1-\alpha)it}{1-it}\right)^n = \left(\frac{\alpha}{1-it} + \frac{(1-\alpha)(1-it)}{1-it}\right)^n = \\
                                   & = \left(\alpha\frac{1}{1-it} + (1-\alpha)\right)^n = \varphi_W(t)^n
              \end{align*}
              donde $\varphi_W(t) = \alpha\frac{1}{1-it} + (1-\alpha)$.
              Luego $X_n = \sum_{i=1}^n W$.

              Observamos que $\frac{1}{1-it}$ es función característica de $Exp(1)$ y 1 es función característica de $\delta(0)$, así que $W$ es mixtura de estas dos distribuciones.
              Por tanto, $X_n$ es la suma de $n$ variables aleatorias con distribuciones mixtura de $Exp(1)$ y $\delta(0)$.
        \item \textbf{Calcular $E(X_n)$ y $V(X_n)$.}
              \begin{align*}
                  E(X_n) & = E\left(\sum_{i=1}^n W\right) = nE(W) = n(\alpha E(Exp(1)) + (1-\alpha)E(\delta(0))) \\
                         & = n\alpha                                                                             \\
                  V(X_n) & = nV(W) = n(E(W^2) - E(W)^2) =                                                        \\
                         & = n(\alpha E(Exp(1)^2) + (1-\alpha)E(\delta(0)^2) - \alpha^2) =                       \\
                         & = n(\alpha(V(Exp(1)) + E(Exp(1))^2) - \alpha^2) =                                     \\
                         & = n(2\alpha - \alpha^2) = n\alpha(2-\alpha)
              \end{align*}

              También se podría haber usado que $\varphi^{(k)}(0) = i^k E(X^k)$.
              $$\varphi_W'(t) = \frac{i\alpha}{(1-it)^2} \qquad \varphi_W''(t) = \frac{-2\alpha}{(1-it)^3}$$
              De esta forma podemos calcular:
              \begin{align*}
                  E(W)   & = \frac{1}{i}\varphi_W'(0) = \alpha     \\
                  E(W^2) & = \frac{1}{i^2}\varphi_W''(0) = 2\alpha
              \end{align*}
        \item \textbf{Determinar el límite en distribución de la sucesión $\{Y_n\}_{n \geq 1}$ con:}
              $$Y_n = \frac{X_n - n\alpha}{\sqrt{n}}$$
              Por el teorema de continuidad de Lévy, basta hallar $\lim\limits_{n \to \infty} \varphi_{Y_n}(t)$, $t \in \mathbb{R}$.
              \begin{align*}
                  \varphi_{Y_n}(t) & = E(e^{itY_n}) = E(e^{it\frac{X_n-n\alpha}{\sqrt{n}}}) = e^{-it\sqrt{n}\alpha}\varphi_{X_n}\left(\frac{t}{\sqrt{n}}\right) = \\
                                   & = e^{-it\sqrt{n}\alpha} \left(\frac{\alpha}{1-i\frac{t}{\sqrt{n}}} + 1-\alpha\right)
              \end{align*}
              Como este límite es difícil de resolver podemos proceder de otra forma.
              Podemos escribir $X_n - n\alpha = \sum_{i=1}^n (W - \alpha) = \sum_{i=1}^n U$, con $U = W-\alpha$.
              Entonces:
              $$\varphi_{Y_n}(t) = \varphi_{X_n - n\alpha} \left(\frac{t}{\sqrt{n}}\right) = \varphi_{\sum_{i=1}^n U} \left(\frac{t}{\sqrt{n}}\right) = \varphi_U \left(\frac{t}{\sqrt{n}}\right)^n$$

              \begin{note}
                  Hemos usado que $\varphi_{aX}(t) = \varphi_X(at)$.
              \end{note}

              Sabemos que $E(U) = 0$ y $E(U^2) = V(U) = V(W) = \alpha(2-\alpha)$.
              También sabemos que:
              $$\varphi_U(t) = \sum_{k=0}^\infty \frac{t^k}{k!} \varphi_U^{(k)}(0) = \sum_{k=0}^\infty \frac{(it)^k}{k!} E(U^k) = 1 + itE(U) - \frac{t^2}{2}E(U^2) + O(t^2)$$
              Luego:
              $$\varphi_V\left(\frac{t}{\sqrt{n}}\right) = 1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n}\right)$$
              Así que:
              \begin{align*}
                  \lim\limits_{n \to \infty} \varphi_{Y_n}(t) & = \lim\limits_{n \to \infty} \varphi_U\left(\frac{t}{\sqrt{n}}\right)^n =                                                                                \\
                                                              & = \lim\limits_{n \to \infty} \left(1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n}\right)\right)^n =                                           \\
                                                              & = e^{\lim\limits_{n \to \infty} n\left(1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n} - 1\right)\right)} = e^{-\frac{t^2}{2}\alpha(2-\alpha)}
              \end{align*}
              Por tanto, $Y_n \xrightarrow{d} Y \sim N(0, \alpha(2-\alpha))$.
    \end{enumerate}
\end{exercise}

\begin{remark}[Teorema central del límite de Lévy-Lindeberg]
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independientes e igualmente distribuidas, con $\mu = E(X_n) < \infty$ y $\sigma^2 = V(X_n) < \infty$.

    Consideramos $Y_n = \frac{X_n - \mu}{\sigma}$, con $E(Y_n) = 0$, $V(Y_n) = 1$ y $\varphi_{Y_n}(t) = 1 - \frac{t^2}{2} + O(t^2)$.
    Sea $Z_n = \frac{1}{\sigma\sqrt{n}} \sum_{i=1}^n (X_i - \mu) = \frac{1}{\sqrt{n}} \sum_{i=1}^n Y_i$. Veamos que $Z_n \xrightarrow{d} N(0, 1)$.
    $$\varphi_{Z_n}(t) = \varphi_{\sum_{j=1}^n Y_j} \left(\frac{t}{\sqrt{n}}\right) = \varphi_{Y_1} \left(\frac{t}{\sqrt{n}}\right)^n = \left(1 - \frac{t^2}{2n} + O\left(\frac{t^2}{n}\right)\right)^n \to e^{-t^2/2}$$
\end{remark}

\begin{lemma}
    $$\left|e^{iy} - \sum_{k=0}^n \frac{(iy)^k}{k!}\right| \leq \min\left\{2\frac{|y|^n}{n!}, \frac{|y|^{n+1}}{(n+1)!}\right\}$$
\end{lemma}