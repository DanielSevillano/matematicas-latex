\chapter{Función característica}
\begin{definition}
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$.
    La función característica asociada a $X$ es:
    $$\varphi_X: \mathbb{R} \to \mathbb{C}$$
    $$\varphi_X(t) = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x)$$
\end{definition}

\begin{remark}
    Usando que $e^{ix} = \cos(x) + i\sin(x)$, podemos escribir:
    $$\varphi_X(t) = \int_\mathbb{R} \cos(tx)dF(x) + i \int_\mathbb{R} \sin(tx)dF(x)$$
\end{remark}

\begin{example}
    Sea $X \sim \delta(a)$, con $a \in \mathbb{R}$.
    Su función característica es:
    $$\varphi_X(t) = E(e^{itX}) = e^{ita} P(X=a) = e^{ita}$$
\end{example}

\begin{example}
    Sea $X \sim Bi(n, p)$, con $n \geq 0$ y $0 \leq p \leq 1$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \sum_{k=0}^n e^{itk} P(X=k) = \sum_{k=0}^n e^{itk} \binom{n}{k} p^k(1-p)^{n-k} = \\
                     & = \sum_{k=0}^n \binom{n}{k} (pe^{it})^k(1-p)^{n-k} = (pe^{it} + 1 - p)^n
    \end{align*}
\end{example}

\begin{example}
    Sea $X \sim Po(\lambda)$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \sum_{k=0}^n e^{itk} P(X=k) = \sum_{k=0}^n e^{itk}e^{-\lambda}\frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^n \frac{(\lambda e^{it})^k}{k!} = \\
                     & = e^{-\lambda}e^{\lambda e^{it}} = e^{\lambda(e^{it} - 1)}
    \end{align*}
\end{example}

\begin{remark}
    $$\varphi_{Bi}(x) = (pe^{it} + 1 - p)^n = \left( 1 + \frac{np(e^{it}-1)}{n} \right)^n \xrightarrow[n \to \infty]{np \to \lambda} e^{\lambda(e^{it} - 1)} = \varphi_{Po}(t)$$
\end{remark}

\begin{example}
    Sea $X \sim U([0, 1])$.
    Su función característica es:
    $$\varphi_X(t) = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x) = \int_\mathbb{R} e^{itx}f_X(x)dx = \int_0^1 e^{itx}dx = \frac{e^{it}-1}{it}$$
\end{example}

\begin{example}
    Sea $X \sim N(0, 1)$.
    Su función característica es:
    \begin{align*}
        \varphi_X(t) & = \int_\mathbb{R} e^{itx}f_X(x)dx = \int_\mathbb{R} e^{itx} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-\frac{(x^2-2itx)}{2}}dx =                     \\
                     & = \frac{1}{\sqrt{2\pi}} \int_\mathbb{R} e^{-\frac{t^2}{2}}e^{-\frac{x^2-t^2-2it}{2}}dx = \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} \int_\mathbb{R} e^{-\frac{(x-it)^2}{2}}dx = e^{-\frac{t^2}{2}}
    \end{align*}

    \begin{note}
        $\int_\mathbb{R} e^{-x^2}dx = \sqrt{\pi}$
    \end{note}
\end{example}

\section{Propiedades}
Veamos las propiedades más importantes de las funciones características.
Sea $\varphi$ la función característica de una variable aleatoria $X$.
Entonces:
\begin{enumerate}
    \item $\varphi(0) = 1$.
          $$E(e^{i0X}) = E(1) = 1$$
    \item $|\varphi(t)| \leq 1$, para todo $t \in \mathbb{R}$.
          $$|\varphi(t)| = |E(e^{itX})| \leq E|e^{itX}| = E|\cos(tx) + i\sin(tx)| = 1$$
    \item $\varphi(-t) = \overline{\varphi(t)}$.
          \begin{align*}
              \varphi(-t) & = E(e^{itX}) = E(\cos(-tx) + i\sin(-tx)) =                             \\
                          & = E(\cos(tx)) - iE(\sin(tx)) = \overline{E(\cos(tx)) + iE(\sin(tx))} = \\
                          & = \overline{E(\cos(tx)) + i\sin(tx)} = \overline{\varphi(t)}
          \end{align*}
    \item $\varphi$ es función definida positiva, es decir,
          $$\sum_{k,j=1}^n z_k\varphi(t_j-t_k)\overline{z_j} \geq 0, \quad \forall n \geq 1, \quad \forall z \in \mathbb{C}^n$$
\end{enumerate}

Además,
\begin{itemize}
    \item $\varphi \in \mathbb{R}^n$ $\Leftrightarrow$ $f$ es simétrica.
          $$\varphi(-t) = \overline{\varphi(t)} = \varphi(t)$$
    \item Sea $Y = a + bX$. Entonces:
          $$\varphi_Y(t) = E(e^{it(a+bX)}) = e^{ita} E(e^{itbX}) = e^{ita} \varphi_X(bt)$$
\end{itemize}

\begin{theorem}
    $\varphi$ es uniformemente continua en $\mathbb{R}$.
\end{theorem}

\section{Teorema de inversión}
\begin{theorem}[Teorema de inversión]
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$ y función característica $\varphi$.
    Sean $a, b \in \mathbb{R}$ con $a < b$, entonces:
    $$\frac{F(b)+F(b^-)}{2} - \frac{F(a)+F(a^-)}{2} = \lim\limits_{T \to \infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-itb}-e^{-ita}}{-it} \varphi(t)dt$$
\end{theorem}

\begin{corollary}
    Sea $X$ variable aleatoria en $(\Omega, \mathcal{A}, P)$ con función de distribución $F$ y función característica $\varphi$.
    Sean $a, b \in C(F)$ con $a < b$, entonces:
    $$F(b) - F(a) = \lim\limits_{T \to \infty} \frac{1}{2\pi} \int_{-T}^T \frac{e^{-itb}-e^{-ita}}{-it} \varphi(t)dt$$
\end{corollary}

\begin{theorem}[Unicidad]
    Sean $X_1$ y $X_2$ variables aleatorias, con funciones de distribución $F_1$ y $F_2$ y funciones características $\varphi_1$ y $\varphi_2$ respectivamente.
    Entonces:
    $$F_1 = F_2 \Leftrightarrow \varphi_1 = \varphi_2$$
\end{theorem}

\begin{theorem}
    Existe $k \in (0, \infty)$ tal que para todo $a > 0$ y toda medida de probabilidad $P_F$ se tiene que:
    $$P_F \left(\left[-\frac{1}{a}, \frac{1}{a}\right]^c\right) \leq \frac{k}{a} \int_0^a (1 - Re(\varphi_F(t)))dt$$
    donde $\varphi_F$ es la función característica asociada a $P_F$.
\end{theorem}

\begin{corollary}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias $X_n$ con $F_n$, $P_{F_n}$ y $\varphi_{F_n}$.
    Supongamos que:
    \begin{enumerate}
        \item Existe $\delta > 0$ tal que $\varphi_{F_n}(t) \xrightarrow[n \to \infty]{} \varphi(t)$ para todo $t \in [-\delta, \delta]$, siendo $\varphi$ una función.
        \item $\varphi$ es continua en 0.
    \end{enumerate}
    Entonces $\{X_n\}_{n \geq 1}$ es una sucesión ajustada.
    Es decir, $\{F_n\}$ forma una familia ajustada.
\end{corollary}

\begin{exercise}
    Sea $X$ con función de densidad:
    $$f(x) = \begin{cases}
            \frac{1}{a} \left(1 - \frac{|x|}{a}\right) & \text{si } |x| \leq a, \; a > 0 \\
            0                                          & \text{en otro caso}
        \end{cases}$$
    Calculemos su función característica $\varphi$.

    Como $f$ es simétrica, sabemos que $\varphi \in \mathbb{R}$.
    \begin{align*}
        \varphi(t) & = E(e^{itX}) = \int_\mathbb{R} e^{itx}dF(x) = \int_\mathbb{R} e^{itx}f(x)dx =                                           \\
                   & = \int_{-a}^a \cos(tx)f(x)dx + \int_{-a}^a \sin(tx)f(x)dx = 2\int_0^a \cos(tx)\frac{1}{a}\left(1-\frac{x}{a}\right)dx = \\
                   & = \frac{2(1-\cos(at))}{a^2t^2} = \frac{\sin^2\left(\frac{at}{2}\right)}{\left(\frac{at}{2}\right)^2}
    \end{align*}

    En el último paso hemos usado que $\sin^2(\frac{\alpha}{2}) = \frac{1-\cos(\alpha)}{2}$.
\end{exercise}

\begin{exercise}
    Sea $X$ con función de densidad:
    $$f(x) = \frac{1}{2\beta}e^{-\frac{|x-\alpha|}{\beta}}, \quad \alpha \in \mathbb{R}, \; \beta > 0$$
    Calculemos su función característica $\varphi_X$.

    Para facilitar los cálculos, consideramos la variable estandarizada $Y = \frac{X-\alpha}{\beta}$ y calculamos $\varphi_Y$.
    Para ello, hallamos primero $F_Y$ y $f_Y$:
    \begin{align*}
        F_Y(y) & = P(Y \leq y) = P\left(\frac{X-\alpha}{\beta} \leq y\right) = P(X \leq \beta y + \alpha) = F_X(\beta y + \alpha) \\
        f_Y(y) & = F_Y'(y) = F_X'(\beta y + \alpha)\beta = f_X(\beta y + \alpha)\beta = \frac{1}{2}e^{-|y|}
    \end{align*}
    Observamos que $f_Y$ es simétrica, así que $\varphi_Y \in \mathbb{R}$.
    \begin{align*}
        \varphi_Y(t) & = \int_\mathbb{R} e^{ity}\frac{1}{2}e^{-|y|}dy = \int_\mathbb{R} (\cos(ty) + i\sin(ty))\frac{1}{2}e^{-|y|}dy = \\
                     & = \int_0^\infty \cos(ty)e^{-y}dy = \frac{1}{1+t^2}
    \end{align*}
    Por tanto:
    $$\varphi_X(t) = e^{it\alpha}\varphi_Y(\beta t) = \frac{e^{it\alpha}}{1+\beta^2t^2}$$

    También se puede ver que $Y = Y_1 - Y_2$, con $Y_i \sim Exp(1)$ independientes.
    De esta forma:
    \begin{align*}
        \varphi_Y(t) & = E(e^{itY}) = E(e^{it(Y_1-Y_2)}) = E(e^{itY_1})E(e^{-itY_2}) = \varphi_{Y_1}(t)\varphi_{Y_2}(-t) = \\
                     & = \varphi_{Y_1}(t) \overline{\varphi_{Y_2}(t)} = \frac{1}{1-it}\frac{1}{1+it} = \frac{1}{1+t^2}
    \end{align*}
\end{exercise}

\section{Teorema de continuidad}
\begin{theorem}[Teorema de continuidad de Lévy]
    Sea $\{X_n\}_{n \geq 1}$ y sean $\varphi_n$ las funciones características asociadas.
    Supongamos que existe una función $\varphi$ tal que:
    \begin{enumerate}
        \item $\varphi_n(t) \to \varphi(t)$, $t \in \mathbb{R}$.
        \item $\varphi$ es continua en 0.
    \end{enumerate}
    Entonces $X_n \xrightarrow{d} X$, donde $X$ es la variable aleatoria con función característica $\varphi$.
\end{theorem}

\begin{theorem}
    Una sucesión $\{F_n\}_{n \geq 1}$ es ajustada si y solo si $$\lim\limits_{t \to 0} \left( \limsup\limits_{n \to \infty} Re(1 - \varphi_n(t))\right) = 0$$
\end{theorem}

\begin{remark}[Teorema central del límite de De Moivre]
    Sean $X_n \sim Bi(n, p)$.
    Sabemos que $E(X_n) = np$ y $V(X_n) = npq$, con $q = 1-p$.
    Consideramos:
    $$Z_n = \frac{X_n - np}{\sqrt{npq}} = \frac{X_n - np}{\sigma_n}$$
    Veamos que $Z_n \xrightarrow{d} Z$.

    Calculamos:
    \begin{align*}
        \varphi_{Z_n}(t) & = E(e^{itZ_n}) = E(e^{it\frac{X_n-np}{\sigma_n}}) = e^{-\frac{itnp}{\sigma_n}} E(e^{\frac{it}{\sigma_n}X_n}) = e^{-\frac{itnp}{\sigma_n}} \varphi_{X_n} \left(\frac{t}{\sigma_n}\right) = \\
                         & = e^{-\frac{itnp}{\sigma_n}} (pe^{i\frac{t}{\sigma_n}} + q)^n = (pe^{i\frac{t}{\sigma_n}q} + qe^{-i\frac{t}{\sigma_n}p})^n
    \end{align*}

    Se puede comprobar que:
    $$\lim\limits_{n \to \infty} \varphi_{Z_n}(t) = \lim\limits_{n \to \infty} (pe^{i\frac{t}{\sigma_n}q} + qe^{-i\frac{t}{\sigma_n}p})^n = e^{-\frac{t^2}{n}} = \varphi_Z(t)$$
    con $Z \sim N(0, 1)$.

    Por el teorema de continuidad de Lévy, $Z_n \xrightarrow{d} Z \sim N(0, 1)$.
\end{remark}

\section{Momentos}
Recordamos los momentos de una variable aleatoria $X$.
\begin{itemize}
    \item Momento de orden $n$: $E(X^n)$.
    \item Momento central de orden $n$: $E((X-E(X))^n)$.
    \item Momento absoluto de orden $n$: $E(|X|^n)$.
    \item Momento central absoluto de orden $n$: $E(|X-E(X)|^n)$.
\end{itemize}

\begin{proposition}
    Si $E(|X|^p) < \infty$ para algún $n \geq 1$, entonces:
    $$E(X^r), E(|X|^r) < \infty, \quad 0 < r \leq n$$
\end{proposition}

\begin{definition}
    El espacio $L^p$ es el conjunto de las variables $X$ tales que $E(|X|^r) < \infty$.
    $$L^r = \left\{X \text{ variable aleatoria } : \int |X|^rdF(x) < \infty\right\}$$

    $(L^p, \|.\|_p)$ es un espacio normado, con $\|X\|_p = (E(|X|^p))^{1/p}$, $X \in L^p$.
\end{definition}

\begin{theorem}
    Sea $X \in L^n$ para algún $n \geq 1$ y con función característica $\varphi$.
    Entonces existen las derivadas $\varphi^{(k)}$ con $k = 1, \dots, n$ y son uniformemente continuas.
    Además,
    $$\varphi^{(k)}(t) = i^k \int x^ke^{itx}dF(x)$$
    y $\varphi^{(k)}(0) = i^kE(X^k)$.
    Así que $\varphi$ se puede expresar como:
    $$\varphi(t) = 1 + \sum_{k=1}^n \frac{(it)^k}{k!}E(X^k) + O(t^n)$$
\end{theorem}

\begin{proposition}
    Sean $X$ e $Y$ variables aleatorias independientes con funciones características $\varphi_X$ y $\varphi_Y$ respectivamente.
    Entonces la función característica de $S = X+Y$ es $\varphi_S(t) = \varphi_X(t)\varphi_Y(t)$.
\end{proposition}

En general, si $X_1, \dots, X_n$ son variables aleatorias independientes, la función característica de $S = X_1 + \dots + X_n$ es:
$$\varphi_S(t) = \prod_{i=1}^n \varphi_{X_i}(t)$$
Si además $X_1, \dots, X_n$ son igualmente distribuidas, entonces $\varphi_S(t) = \varphi_{X_k}(t)^n$ para cualquier $k \in \{1, \dots, n\}$.

\begin{exercise}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independeintes, cada una con función característica:
    $$\varphi_{X_n}(t) = \left(\frac{1 - (1-\alpha)it}{1-it}\right), \quad 0 \leq \alpha \leq 1$$

    \begin{enumerate}
        \item \textbf{Determinar la distribución de $X_n$.}
              \begin{align*}
                  \varphi_{X_n}(t) & = \left(\frac{1 - \alpha + \alpha - (1-\alpha)it}{1-it}\right)^n = \left(\frac{\alpha}{1-it} + \frac{(1-\alpha)(1-it)}{1-it}\right)^n = \\
                                   & = \left(\alpha\frac{1}{1-it} + (1-\alpha)\right)^n = \varphi_W(t)^n
              \end{align*}
              donde $\varphi_W(t) = \alpha\frac{1}{1-it} + (1-\alpha)$.
              Luego $X_n = \sum_{i=1}^n W$.

              Observamos que $\frac{1}{1-it}$ es función característica de $Exp(1)$ y 1 es función característica de $\delta(0)$, así que $W$ es mixtura de estas dos distribuciones.
              Por tanto, $X_n$ es la suma de $n$ variables aleatorias con distribuciones mixtura de $Exp(1)$ y $\delta(0)$.
        \item \textbf{Calcular $E(X_n)$ y $V(X_n)$.}
              \begin{align*}
                  E(X_n) & = E\left(\sum_{i=1}^n W\right) = nE(W) = n(\alpha E(Exp(1)) + (1-\alpha)E(\delta(0))) \\
                         & = n\alpha                                                                             \\
                  V(X_n) & = nV(W) = n(E(W^2) - E(W)^2) =                                                        \\
                         & = n(\alpha E(Exp(1)^2) + (1-\alpha)E(\delta(0)^2) - \alpha^2) =                       \\
                         & = n(\alpha(V(Exp(1)) + E(Exp(1))^2) - \alpha^2) =                                     \\
                         & = n(2\alpha - \alpha^2) = n\alpha(2-\alpha)
              \end{align*}

              También se podría haber usado que $\varphi^{(k)}(0) = i^k E(X^k)$.
              $$\varphi_W'(t) = \frac{i\alpha}{(1-it)^2} \qquad \varphi_W''(t) = \frac{-2\alpha}{(1-it)^3}$$
              De esta forma podemos calcular:
              \begin{align*}
                  E(W)   & = \frac{1}{i}\varphi_W'(0) = \alpha     \\
                  E(W^2) & = \frac{1}{i^2}\varphi_W''(0) = 2\alpha
              \end{align*}
        \item \textbf{Determinar el límite en distribución de la sucesión $\{Y_n\}_{n \geq 1}$ con:}
              $$Y_n = \frac{X_n - n\alpha}{\sqrt{n}}$$
              Por el teorema de continuidad de Lévy, basta hallar $\lim\limits_{n \to \infty} \varphi_{Y_n}(t)$, $t \in \mathbb{R}$.
              \begin{align*}
                  \varphi_{Y_n}(t) & = E(e^{itY_n}) = E(e^{it\frac{X_n-n\alpha}{\sqrt{n}}}) = e^{-it\sqrt{n}\alpha}\varphi_{X_n}\left(\frac{t}{\sqrt{n}}\right) = \\
                                   & = e^{-it\sqrt{n}\alpha} \left(\frac{\alpha}{1-i\frac{t}{\sqrt{n}}} + 1-\alpha\right)
              \end{align*}
              Como este límite es difícil de resolver podemos proceder de otra forma.
              Podemos escribir $X_n - n\alpha = \sum_{i=1}^n (W - \alpha) = \sum_{i=1}^n U$, con $U = W-\alpha$.
              Entonces:
              $$\varphi_{Y_n}(t) = \varphi_{X_n - n\alpha} \left(\frac{t}{\sqrt{n}}\right) = \varphi_{\sum_{i=1}^n U} \left(\frac{t}{\sqrt{n}}\right) = \varphi_U \left(\frac{t}{\sqrt{n}}\right)^n$$

              \begin{note}
                  Hemos usado que $\varphi_{aX}(t) = \varphi_X(at)$.
              \end{note}

              Sabemos que $E(U) = 0$ y $E(U^2) = V(U) = V(W) = \alpha(2-\alpha)$.
              También sabemos que:
              $$\varphi_U(t) = \sum_{k=0}^\infty \frac{t^k}{k!} \varphi_U^{(k)}(0) = \sum_{k=0}^\infty \frac{(it)^k}{k!} E(U^k) = 1 + itE(U) - \frac{t^2}{2}E(U^2) + O(t^2)$$
              Luego:
              $$\varphi_V\left(\frac{t}{\sqrt{n}}\right) = 1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n}\right)$$
              Así que:
              \begin{align*}
                  \lim\limits_{n \to \infty} \varphi_{Y_n}(t) & = \lim\limits_{n \to \infty} \varphi_U\left(\frac{t}{\sqrt{n}}\right)^n =                                                                                \\
                                                              & = \lim\limits_{n \to \infty} \left(1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n}\right)\right)^n =                                           \\
                                                              & = e^{\lim\limits_{n \to \infty} n\left(1 - \frac{t^2}{2n}\alpha(2-\alpha) + O\left(\frac{t^2}{n} - 1\right)\right)} = e^{-\frac{t^2}{2}\alpha(2-\alpha)}
              \end{align*}
              Por tanto, $Y_n \xrightarrow{d} Y \sim N(0, \alpha(2-\alpha))$.
    \end{enumerate}
\end{exercise}

\begin{remark}[Teorema central del límite de Lévy-Lindeberg]
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independientes e igualmente distribuidas, con $\mu = E(X_n) < \infty$ y $\sigma^2 = V(X_n) < \infty$.

    Consideramos $Y_n = \frac{X_n - \mu}{\sigma}$, con $E(Y_n) = 0$, $V(Y_n) = 1$ y $\varphi_{Y_n}(t) = 1 - \frac{t^2}{2} + O(t^2)$.
    Sea $Z_n = \frac{1}{\sigma\sqrt{n}} \sum_{i=1}^n (X_i - \mu) = \frac{1}{\sqrt{n}} \sum_{i=1}^n Y_i$. Veamos que $Z_n \xrightarrow{d} N(0, 1)$.
    $$\varphi_{Z_n}(t) = \varphi_{\sum_{j=1}^n Y_j} \left(\frac{t}{\sqrt{n}}\right) = \varphi_{Y_1} \left(\frac{t}{\sqrt{n}}\right)^n = \left(1 - \frac{t^2}{2n} + O\left(\frac{t^2}{n}\right)\right)^n \to e^{-t^2/2}$$
\end{remark}

\begin{lemma}
    $$\left|e^{iy} - \sum_{k=0}^n \frac{(iy)^k}{k!}\right| \leq \min\left\{2\frac{|y|^n}{n!}, \frac{|y|^{n+1}}{(n+1)!}\right\}$$
\end{lemma}

\begin{theorem}
    Si $\varphi$ es absolutamente integrable, entonces $F$ es absolutamente continua con densidad:
    $$f(x) = \frac{1}{2\pi} \int_\mathbb{R} e^{-itx}\varphi(t)dt$$
\end{theorem}

\begin{theorem}[Lema de Riemman-Lebesgue]
    Si $F$ es absolutamente continua, entonces:
    $$\lim\limits_{|t| \to \infty} \varphi(t) = 0$$
\end{theorem}

\begin{definition}
    Sea $X$ una variable aleatoria con función característica $\varphi$.
    Definimos su función generatriz de cumulantes como:
    $$K: \mathbb{R} \to \mathbb{C}, \quad K(t) = \log(\varphi(t))$$
\end{definition}

\begin{proposition}
    Sean $X_1, \dots, X_n$ variables aleatorias con funciones características $\varphi_1, \dots, \varphi_n$ y sea $S = X_1 + \dots + X_n$.
    Entonces:
    $$K_S(t) = \sum_{i=1}^n K_{x_i}(t)$$
\end{proposition}

\begin{theorem}
    Sea $X$ una variable aleatoria.
    Supongamso que $E(|X|^n) < \infty$ para algún $n \geq 1$.
    Entonces:
    $$K_X(t) = \sum_{j=0}^n \frac{(it)^j}{j!}c_j + O(t^n)$$
    donde $c_j = \frac{K^{(j)}(0)}{i^j}$ es el cumulante de orden $j$.
\end{theorem}

\begin{note}
    \begin{align*}
        c_1 & = \frac{K'(0)}{i} = \frac{1}{i} \frac{\varphi'(0)}{\varphi(0)} = \frac{1}{i}\varphi'(0) = E(X)                                                       \\
        c_2 & = \frac{K''(0)}{i^2} = -K''(0) = -\left(\frac{\varphi''(0)}{\varphi(0)} - \left(\frac{\varphi'(0)}{\varphi(0)}\right)^2\right) = (E(X^2) - E(X)^2) = \\
            & = V(X)
    \end{align*}
\end{note}

\begin{definition}
    Sea $X$ una variable aleatoria y $\sigma = \sqrt{V(X)}$.
    \begin{itemize}
        \item Definimos el sesgo de $X$ como $\dfrac{c_3}{\sigma^3}$.
        \item Definimos la curtosis de $X$ como $\dfrac{c_4}{\sigma^4}$.
    \end{itemize}
\end{definition}

\begin{definition}
    Sea $X$ una variable aleatoria con función característica $\varphi$.
    Definimos la función de generatriz de momentos de $X$ como:
    $$\psi(t) = E(e^{tX}) = \int_\mathbb{R} e^{tx}dF(x)$$
    $\psi$ está definida en un entorno del 0.
\end{definition}

\begin{remark}
    Si existe $\psi$ entonces $E(|X|^n) < \infty$ para todo $n \geq 1$.
\end{remark}

\begin{definition}
    Sea $X$ una variable aleatoria con valores en $\mathbb{Z}_+ = \{0, 1, 2, \dots\}$.
    La función generatriz de probabilidad de $X$ es:
    $$G_X(t) = E(t^X) = \sum_{n=0}^\infty t^nP(X=n), \quad |t| < 1$$
\end{definition}

\begin{remark}
    $$G_x^{(k)}(0) = k!P(X=k) \Rightarrow P(X=k) = \frac{G_X^{(k)}(0)}{k!}$$
\end{remark}

Sea $X = Y_1 + \dots + Y_N$, con $Y_i$ variables aleatorias independientes e idénticamente distribuidas y $N$ una variable aleatoria en $\mathbb{Z}_+$.
$X$ sigue una distribución compuesta.
\begin{align*}
    \varphi_X(t) & = E(e^{itX}) = E(E(e^{itX}|N)) = \sum_{n=0}^\infty E(e^{itX}|N=n)P(N=n) =                                        \\
                 & = \sum_{n=0}^\infty E(e^{it\sum_{i=1}^n X_i})P(N=n) = \sum_{n=0}^\infty \varphi_Y(t)^nP(N=n) = G_N(\varphi_Y(t))
\end{align*}

\begin{example}
    Sea $X = Y_1 + \dots + Y_N$, con $Y_i$ variables aleatorias independientes e idénticamente distribuidas y $N \sim Po(\lambda)$ una variable aleatoria en $\mathbb{Z}_+$.
    $$\varphi_X(t) = G_N(\varphi_Y(t)) = e^{\lambda(\varphi_Y(t)-1)}$$
    donde
    $$G_N(t) = E(t^N) = \sum_{n=0}^\infty t^nP(N=n) = \sum_{n=0}^\infty t^ne^{-\lambda}\frac{\lambda^n}{n!} = e^{\lambda(t-1)}$$
    Además,
    $$K_X(t) = \log(\varphi_X(t)) = \lambda(\varphi_Y(t) - 1)$$
    Luego podemos calcular:
    $$E(X) = c_1 = \frac{K'(0)}{i} = \frac{1}{i}\lambda \varphi_Y'(0) = \lambda E(Y)$$
\end{example}

\section{Reconocimiento de funciones características}
Para identificar funciones características usamos alguna de las siguientes estrategias:
\begin{itemize}
    \item Reconocer la función característica de alguna distribución conocida.
    \item Encontrar una variable aleatoria cuya función característica sea la que buscamos.
    \item Usar otros resultados.
\end{itemize}

\begin{exercise}
    Supongamos que $\varphi_X$ es una función característica.
    Veamos que $|\varphi_X|^2$ también lo es.
    $$|\varphi_X(t)|^2 = \varphi_X(t) \overline{\varphi_X(t)} = \varphi_X(t)\varphi_{-X}(t), \quad t \in \mathbb{R}$$
    Definimos $X'$ como una copia independiente de $-X$.Entonces, la variable $Y = X + X'$ tiene como función característica a $|\varphi_X|^2$.
\end{exercise}

\begin{lemma}
    Sean $\mu_1, \dots, \mu_n$ medidas de probabilidad con funciones características $\varphi_1, \dots, \varphi_n$ respectivamente.
    Sean $\alpha_1, \dots, \alpha_n \in [0, 1]$ tales que $\sum_{i=1}^n \alpha_i = 1$.
    Entonces la función característica asociada a la medida $\sum_{i=1}^n \alpha_i\mu_i$ es:
    $$\varphi(t) = \sum_{i=1}^n \alpha_i\varphi_i(t)$$
    Es decir, toda combinación lineal convexa de funciones características es una función característica.
\end{lemma}

\begin{exercise}
    Supongamos que $\varphi_X$ es una función característica.
    Veamos que $Re(\varphi_X)$ también lo es.
    $$Re(\varphi_X) = \frac{\varphi_X + \overline{\varphi_X}}{2} = \frac{1}{2}\varphi_X + \frac{1}{2}\varphi_{X'}$$
    donde $X'$ es una copia independiente de $-X$.

    Definimos la variable aleatoria $Y$ cuya distribución de probabilidad es una mixta de las distribuciones $X$ y $X'$ con pesos $\frac{1}{2}$ y $\frac{1}{2}$.
    Entonces, $Y$ tiene como función característica $\varphi_Y(t) = Re(\varphi_X(t))$.
\end{exercise}

\begin{definition}
    Una función $g: \mathbb{R} \to \mathbb{C}$ es definida positiva si:
    $$\sum_{j=1}^n \sum_{k=1}^n g(t_j-t_k)z_j\bar{z}_k \geq 0$$
    para todo $t_1, \dots, t_n \in \mathbb{R}$ y $z_1, \dots, z_n \in \mathbb{C}$.
\end{definition}

\begin{remark}
    La función característica es definida positiva.
\end{remark}

\begin{theorem}
    Sea $g$ una función definida positiva.
    Si $g$ es continua en 0 entonces es uniformemente continua en $\mathbb{R}$.
\end{theorem}

\begin{lemma}[Herglotz]
    Sea $\phi: \mathbb{Z} \to \mathbb{C}$ definida positiva con $\phi(0) = 1$.
    Entonces existe $\mu$ distribución de probabilidad en $[-\pi, \pi]$ tal que $\phi$ es su función característica asociada, es decir,
    $$\phi(t) = \int_{-\pi}^\pi e^{itx}\mu(dx), \quad \forall t \in \mathbb{Z}$$
\end{lemma}

\begin{theorem}[Bochner]
    Sea $\varphi: \mathbb{R} \to \mathbb{C}$ tal que:
    \begin{enumerate}
        \item $\varphi$ es definida positiva.
        \item $\varphi$ es continua en 0.
        \item $\varphi(0) = 1$.
    \end{enumerate}
    Entonces $\varphi$ es una función característica.
\end{theorem}

\begin{proposition}
    La función $\varphi_T$ dada por:
    $$\varphi(t) = \max\left\{1-\frac{|t|}{T}, 0\right\} = \begin{cases}
            1 - \frac{|t|}{T} & \text{si } |t| \leq T \\
            0                 & \text{si } |t| > T
        \end{cases}$$
    es una función característica.
\end{proposition}

\begin{lemma}
    Sea $\varphi: \mathbb{R} \to \mathbb{R}$ tal que:
    \begin{enumerate}
        \item $\varphi(0) = 1$.
        \item $\varphi(t) \geq 0 \; \forall t \in \mathbb{R}$.
        \item $\varphi$ es par.
        \item $\varphi$ es una poligonal convexa no creciente en $\mathbb{R}_+$.
    \end{enumerate}
    Entonces $\varphi$ es una función característica.
\end{lemma}

\begin{theorem}[Criterio de Pólya]
    Sea $\varphi: \mathbb{R} \to \mathbb{R}$ tal que:
    \begin{enumerate}
        \item $\varphi(0) = 1$.
        \item $\varphi$ es no negativa, par y continua.
        \item $\varphi$ es convexa y no creciente en $\mathbb{R}_+$.
    \end{enumerate}
    Entonces $\varphi$ es función característica.
\end{theorem}

\begin{exercise}
    Sea $\varphi$ la función dada por:
    $$\varphi(t) = \begin{cases}
            1 - 0.025|t| & \text{si } |t| < 2        \\
            0.9 - 0.2|t| & \text{si } 2 \leq |t| < 3 \\
            0.6 - 0.1|t| & \text{si } 3 \leq |t| < 4 \\
            0.2          & \text{si } |t| \geq 4
        \end{cases}$$
    Veamos que $\varphi$ es función característica.

    Para ello expresamos $\varphi$ como combinación lineal convexa de funciones características de la forma $1 - \frac{|t|}{T}$ en $|t| < T$.
    Escribimos $\varphi$ como:
    $$\varphi(t) = \alpha_1\varphi_2(t) + \alpha_2\varphi_3(t) + \alpha_3\varphi_4(t) + \alpha_4, \quad \varphi_a(t) = \begin{cases}
            1 - \frac{|t|}{a} & \text{si } |t| \leq a \\
            0                 & \text{si } |t| > a
        \end{cases}$$
    Ahora encontramos los $\alpha_i$.
    $$\begin{cases}
            \varphi(0) = 1 = \alpha_1 + \alpha_2 + \alpha_3 + \alpha_4                                                                                               \\
            \varphi(2) = \alpha_2\varphi_3(2) + \alpha_3\varphi_4(2) + \alpha_4 = \alpha_2\left(1-\frac{2}{3}\right) + \alpha_3\left(1-\frac{1}{2}\right) + \alpha_4 \\
            \varphi(3) = \alpha_3\varphi_4(2) + \alpha_1 = \alpha_3\left(1-\frac{3}{4}\right) + \alpha_4                                                             \\
            \varphi(4) = \alpha_4
        \end{cases}$$
    Resolviendo el sistema obtenemos que:
    $$\alpha_1 = 0.1, \quad \alpha_2 = 0.3, \quad \alpha_3 = 0.4, \quad \alpha_4 = 0.2$$

    Por tanto,
    $$\varphi(t) = 0.1\varphi_2(t) + 0.3\varphi_3(t) + 0.4\varphi_4(t) + 0.2$$
    Como cada $\varphi_a$ es función característica, $\varphi$ es función característica por ser combinación lineal convexa de ellas.
\end{exercise}