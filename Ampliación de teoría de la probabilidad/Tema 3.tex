\chapter{Convergencia}
Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad, con $P: \mathcal{A} \to [0, 1]$.
Estudiaremos las sucesiones $\{A_n\}_{n \geq 1}$ con $A_i \in \mathcal{A}$ para todo $i \geq 1$.

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    \begin{itemize}
        \item Definimos el límite superior de la sucesión como:
              $$\limsup\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} \bigcup_{m \geq n} A_m \in \mathcal{A}$$
        \item Definimos el límite inferior de la sucesión como:
              $$\liminf\limits_{n \to \infty} A_n = \bigcup_{n \geq 1} \bigcap_{m \geq n} A_m \in \mathcal{A}$$
    \end{itemize}
\end{definition}

\begin{remark}
    $$\liminf\limits_{n \to \infty} A_n \subseteq \limsup\limits_{n \to \infty} A_n$$
\end{remark}

La sucesión $\{A_n\}_n$ converge si:
$$\liminf\limits_{n \to \infty} A_n = \limsup\limits_{n \to \infty} A_n = \lim\limits_{n \to \infty} A_n$$

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    $\{A_n\}_n$ es monótona creciente si:
    $$A_1 \subseteq A_2 \subseteq \dots \subseteq A_n \subseteq \dots$$

    En ese caso,
    $$\lim\limits_{n \to \infty} A_n = \bigcup_{n \geq 1} A_n$$
\end{definition}

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    $\{A_n\}_n$ es monótona decreciente si:
    $$A_1 \supseteq A_2 \supseteq \dots \supseteq A_n \supseteq \dots$$

    En ese caso,
    $$\lim\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} A_n$$
\end{definition}

\begin{theorem}
    Sea $\{A_n\}_n \subset \mathcal{A}$ monótona.
    Entonces:
    $$P(\lim\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(A_n)$$
\end{theorem}

\begin{theorem}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    Entonces:
    \begin{enumerate}
        \item $$P(\limsup\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(\bigcup_{m \geq n} A_m)$$
        \item $$P(\liminf\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(\bigcap_{m \geq n} A_m)$$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Sean $\{A_n\}_n \subset \mathcal{A}$ y $\omega \in \Omega$.
    Entonces:
    \begin{enumerate}
        \item $w \in \limsup\limits_{n \to \infty} A_n$ si y solo si existe una sucesión de índices
              $$n_1 < n_2 < \dots < n_k < \dots$$
              tal que $w \in A_{n_k}$, para $k = 1, 2, \dots$.
        \item $w \in \liminf\limits_{n \to \infty} A_n$ si y solo si existe $n_0 \geq 1$ tal que $w \in A_m$ para todo $m \geq n_0$.
    \end{enumerate}
\end{theorem}

\begin{theorem}[Primer lema de Borel-Cantelli]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_n \subset \mathcal{A}$ tal que $\sum_{n \geq 1} P(A_n) < \infty$.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 0$$
\end{theorem}

Veamos que el recíproco del primer lema de Borel-Cantelli no es cierto.
\begin{example}
    Sea $(\Omega, \mathcal{A}, P)$ el espacio de probabilidad dado por $\Omega = (0, 1)$, $A = \mathcal{B}_\Omega$ y $P$ la medida de Lebesgue.
    Consideramos la sucesión $\{A_n\}_n$ con $A_n = \left\{\left(0, \frac{1}{\sqrt{n}}\right)\right\}$.
    Observamos que:
    $$\limsup\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} \bigcup_{m \geq n} \left(0, \frac{1}{\sqrt{n}}\right) = \emptyset \Rightarrow P(\limsup\limits_{n \to \infty} A_n) = 0$$
    Sin embargo,
    $$\sum_{n=1}^\infty P(A_n) = \sum_{i=1}^\infty \frac{1}{\sqrt{n}} = \infty$$
\end{example}

\begin{remark}
    \begin{align*}
        \limsup\limits_{n \to \infty} I_{A_n}(\omega) & = I_{\limsup\limits_{n \to \infty} A_n}(\omega) \\
        \liminf\limits_{n \to \infty} I_{A_n}(\omega) & = I_{\liminf\limits_{n \to \infty} A_n}(\omega)
    \end{align*}
\end{remark}

\begin{theorem}[Segundo lema de Borel-Cantelli]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_n \subset \mathcal{A}$ con $A_i$ independientes tal que $\sum_{n \geq 1} P(A_n) = \infty$.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 1$$
\end{theorem}

\begin{remark}
    $$P(\liminf\limits_{n \to \infty} A_n) \leq \liminf\limits_{n \to \infty} P(A_n) \leq \limsup\limits_{n \to \infty} P(A_n) \leq P(\limsup\limits_{n \to \infty} A_n)$$
\end{remark}

\begin{example}
    Un mono pulsando teclas al azar sobre un teclado durante un periodo de tiempo infinito escribirá el Quijote y cualquier texto un número infinito de veces.
\end{example}

\begin{corollary}[Ley 0-1 de Borel-Cantelli]
    Sea $\{A_n\}_n \subset \mathcal{A}$ con $A_i$ independientes.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 0 \quad \text{o} \quad P(\limsup\limits_{n \to \infty} A_n) = 1$$
\end{corollary}

\section{Tipos de convergencia}
Sea $X$ variable aleatoria en el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, definimos:
$$\Omega_1 = \{ \omega \in \Omega : \liminf\limits_{n \to \infty} X_n(\omega) = \limsup\limits_{n \to \infty} X_n(\omega) \}$$

Recordamos que:
\begin{itemize}
    \item Sean $X: \Omega \to S$ y $f: S \to T$ dos funciones medibles.
          Entonces $f(X)$ es medible.
    \item Sean $X_1, \dots, X_n$ variables aleatorias y $f: \mathbb{R}^n \to \mathbb{R}$ una función.
          Entonces $f(X_1, \dots, X_n)$ es variable aleatoria.
    \item Toda función continua es medible Borel.
\end{itemize}

\begin{theorem}
    Sea ${X_n}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Entonces son variables aleatorias:
    \begin{itemize}
        \item $\inf X_n$
        \item $\sup X_n$
        \item $\liminf\limits_{n \to \infty} X_n$
        \item $\limsup\limits_{n \to \infty} X_n$
    \end{itemize}
\end{theorem}

\begin{corollary}
    $\Omega_1$ es medible.
\end{corollary}

\begin{definition}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge casi seguro si $P(\Omega_1) = 1$.

    En tal caso escribimos $X_n \xrightarrow{cs} X$, con $X = \limsup\limits_{n \to \infty} X_n$.
\end{definition}

\begin{theorem}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independientes en $(\Omega, \mathcal{A}, P)$.
    Entonces las siguientes afirmaciones son equivalentes:
    \begin{enumerate}
        \item $X_n \xrightarrow{cs} X$.
        \item $P(\liminf\limits_{n \to \infty} Y_{n, k}) = 1$ para todo $k \geq 1$.
        \item $P(\limsup\limits_{n \to \infty} Y_{n, k}^c) = 0$ para todo $k \geq 1$.
    \end{enumerate}
    donde
    $$Y_{n, k} = \left\{ \omega \in \Omega : |X_n(\omega) - X(\omega)| < \frac{1}{k} \right\}$$
\end{theorem}

\begin{example}
    Veamos un contrajemplo para el caso en el que no hay independencia.

    Sea $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = \begin{cases}
            1 & \text{si } 0 \leq \omega \leq \frac{1}{n} \\
            0 & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Veamos que las variables aleatorias no son independientes dos a dos:
    \begin{align*}
        P(X_n = 1 | X_{n-1} = 0) & = \frac{P((X_n = 1) \cap (X_{n-1} = 0))}{P(X_{n-1} = 0)} = \frac{m\left([0, \frac{1}{n}] \cap (\frac{1}{n-1}, 1]\right)}{1 - \frac{1}{n-1}} = 0 \\
        P(X_n = 1)               & = \frac{1}{n}
    \end{align*}
    Calculamos el límite de la sucesión:
    $$\lim\limits_{n \to \infty} X_n(\omega) = \begin{cases}
            1 & \text{si } \omega = 0        \\
            0 & \text{si } 0 < \omega \leq 1
        \end{cases}$$
    Sea $X(\omega) = 0$, $X_n \xrightarrow{cs} X$ porque:
    $$P(\{\lim\limits_{n \to \infty} X_n \neq X\}) = P(\{0\}) = 0$$
    Sea $\varepsilon > 0$,
    $$Y_{n, 1/\varepsilon}^c = \left\{ \omega \in [0, 1] : |X_n(\omega) - 0| \geq \varepsilon \right\}$$
    Sin embargo, observamos que:
    $$\sum_{n=1}^\infty P(Y_{n, 1/\varepsilon}^c) = \sum_{n=1}^\infty P(|X_n| \geq \varepsilon) = \sum_{n=1}^\infty P(X_n = 1) = \sum_{n=1}^\infty \frac{1}{n} = \infty$$
\end{example}

\begin{definition}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge en probabilidad a $X$ si para todo $\varepsilon > 0$
    $$P(Y_{n, \varepsilon}) \xrightarrow[n \to \infty]{} 1$$

    En tal caso escribimos $X_n \xrightarrow{p} X$.
\end{definition}

\begin{theorem}
    El límite en probabilidad es único en casi todo punto.
\end{theorem}

\begin{theorem}
    Si $X_n \xrightarrow{cs} X$, entonces $X_n \xrightarrow{p} X$.
\end{theorem}

\begin{example}
    Sea $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = \omega^n$$
    Calculamos el límite de la sucesión:
    $$\lim\limits_{n \to \infty} X_n(\omega) = \begin{cases}
            0 & \text{si } 0 \leq \omega < 1 \\
            1 & \text{si } \omega = 1
        \end{cases}$$
    Sea $X(\omega) = 0$, $X_n \xrightarrow{cs} X$ porque:
    $$P(\{\lim\limits_{n \to \infty} X_n \neq X\}) = P(\{1\}) = 0$$

    Como $X_n$ converge casi seguro, sabemos que $X_n$ converge en probabilidad.
    Veamos que esto es cierto.

    Sea $\varepsilon > 0$,
    $$Y_{n, \varepsilon} = \{|X_n - X| < \varepsilon\} = \{X_n < \varepsilon\}$$
    Observamos que:
    $$F_n(x) = P(X_n \leq x) = \begin{cases}
            0           & \text{si } x < 0        \\
            \sqrt[n]{x} & \text{si } 0 \leq x < 1 \\
            1           & \text{si } x \geq 1
        \end{cases}$$
    Por tanto, si tomamos $\varepsilon \leq 1$,
    $$P(Y_{n, \varepsilon}) = F_n(\varepsilon^-) = \sqrt[n]{\varepsilon} \xrightarrow[n \to \infty]{} 1$$
\end{example}

\begin{example}
    Sean $X_n$ variables aleatorias independientes de Bernoulli en un espacio de medida $(\Omega, \mathcal{A}, P)$.
    $$X_n(\omega) = \begin{cases}
            1 & \text{si hay éxito en la prueba } n    \\
            0 & \text{si no hay éxito en la prueba } n
        \end{cases}$$
    Observamos que:
    $$P(X_n = x) = \begin{cases}
            \frac{1}{n}     & \text{si } x = 1    \\
            1 - \frac{1}{n} & \text{si } x = 0    \\
            0               & \text{en otro caso}
        \end{cases}$$

    Sea $X = 0$, veamos que $X_n$ no converge casi seguro a $X$.
    Sea $\varepsilon > 0$,
    $$\sum_{n=1}^\infty P(Y_{n, \varepsilon}^c) = \sum_{n=1}^\infty P(X_n \geq \varepsilon) = \sum_{n=1}^\infty \frac{1}{n} = \infty$$
    Veamos que aún así $X_n$ converge en probabilidad a $X$.
    $$P(Y_{n, \varepsilon}^c) = P(X_n \geq \varepsilon) = \begin{cases}
            \frac{1}{n} & \text{si } 0 < \varepsilon \leq 1 \\
            0           & \text{si } \varepsilon > 1
        \end{cases} \Rightarrow \lim\limits_{n \to \infty} P(Y_{n, \varepsilon}^c) = 0 \quad \forall \varepsilon$$
    Por tanto, $X_n \xrightarrow{p} X$.
\end{example}

\begin{theorem}
    Si $X_n \xrightarrow{p} X$, entonces existe una subsucesión $\{X_{n_k}\}$ tal que $X_{n_k} \xrightarrow[k \to \infty]{cs} X$.
\end{theorem}

\begin{theorem}
    $X_n \xrightarrow{p} X$ si y solo si toda subsucesión contiene una subsucesión convergente casi seguro.
\end{theorem}

\begin{theorem}
    La convergencia en probabilidad implica la convergencia en distribución.
\end{theorem}

\begin{theorem}
    Sean $X_n$ y $X$ en $(\Omega, \mathcal{A}, P)$ con $X \sim \delta(c)$ y $c$ constante.
    Entonces la convergencia en probabilidad es equivalente a la convergencia en distribución.
\end{theorem}

\subsection*{Convergencia en $L^p$}
Dado $0 < p < \infty$, definimos:
$$L^p(\Omega, \mathcal{A}, P) = \{X: \Omega \to \mathbb{R} : E(|X|^p) < \infty\}$$

\begin{note}
    Los elementos de $L^p$ son en realidad clases de equivalencia, con la relación de equivalencia dada por:
    $$X \sim Y \Leftrightarrow X = Y \text{ en casi todo punto}$$
\end{note}

\begin{note}
    Si $X$ es una variable aleatoria que verifica $E(|X|^p) < \infty$, se dice que es $p$-integrable.
    Esta condición es equivalente a que:
    $$\int_\Omega |X|^p dP < \infty$$
\end{note}

Para $1 \leq p < \infty$ podemos definir la norma:
$$\|X\|_p = \left(\int_\Omega |X|^p dP\right)^{1/p}$$
Si $0 < p < 1$, $\|.\|$ es una pseudonorma.

Esta norma induce una métrica:
\begin{align*}
    d: L^p \times L^p & \to \mathbb{R}^+ \\
    d(X, Y)           & = \|X-Y\|_p
\end{align*}

\begin{definition}
    Sean $X_n, X \in L^p(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge a $X$ en $L^p$ si:
    $$\|X_n - X\|_p \xrightarrow[n \to \infty]{} 0$$
    Equivalentemente, si:
    $$E(|X_n - X|^p) \xrightarrow[n \to \infty]{} 0$$

    En tal caso escribimos $X_n \xrightarrow{L^p} X$.
\end{definition}

\begin{example}
    Sean $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = n I_{[0, \frac{1}{n}]}(\omega) = \begin{cases}
            n & \text{si } 0 \leq \omega \leq \frac{1}{n} \\
            0 & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Veamos si $X_n$ converge a $X$ en $L^p$.
    \begin{align*}
        E(|X_n - X|^p) & = E(|X_n|^p) = 0^pP(X_n = 0) + n^pP(X_n = n) = n^pm\left(\left[0, \frac{1}{n}\right]\right) = \\
                       & = \frac{n^p}{n} = n^{p-1} \xrightarrow[n \to \infty]{} \begin{cases}
            0      & \text{si } p < 1 \\
            1      & \text{si } p = 1 \\
            \infty & \text{si } p > 1
        \end{cases}
    \end{align*}
    Luego $X_n$ converge a $X$ en $L^p$ si $p < 1$.
\end{example}

\begin{example}
    Sean $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = 2^n I_{[0, \frac{1}{n}]}(\omega) = \begin{cases}
            2^n & \text{si } 0 \leq \omega \leq \frac{1}{n} \\
            0   & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Sea $X \sim \delta(0)$, se puede comprobar que $X_n \xrightarrow{p} X$.
    Veamos si $X_n$ converge a $X$ en $L^p$.
    \begin{align*}
        E(|X_n - X|^p) & = E(|X_n|^p) = 0^pP(X_n = 0) + 2^{np}P(X_n = 2^n) = 2^{np}m\left(\left[0, \frac{1}{n}\right]\right) = \\
                       & = \frac{2^{np}}{n} \xrightarrow[n \to \infty]{} \infty
    \end{align*}
    Luego $X_n$ no converge a $X$ en $L^p$.
\end{example}

\begin{proposition}[Desigualdad de Márkov]
    Sean $X$ no negativa y $a > 0$.
    Entonces:
    $$P(X \geq a) \leq \frac{E(X)}{a}$$
    Si $X \in L^p$,
    $$P(X \geq a) \leq \frac{E(X^p)}{a^p}$$
\end{proposition}

\begin{remark}
    Si $X \in L^p$ cualquiera, $|X|$ es no negativa luego:
    $$P(|X| \geq a) \leq \frac{E(|X|^p)}{a^p}$$
\end{remark}

\begin{theorem}
    Sean $X_n, X \in L^p$, con $0 < p < \infty$.
    Si $X_n \xrightarrow{L^p} X$, entonces $X_n \xrightarrow{p} X$.
\end{theorem}

\begin{theorem}
    El límite en $L^p$ es único.
\end{theorem}

\begin{theorem}
    Si $X_n \xrightarrow{p} X$ con $X_n, X \in L^p$ y existe $Y \in L^p$ tal que $|X_n| \leq Y$ para todo $n \in \mathbb{N}$, entonces $X_n \xrightarrow{L^p} X$.
\end{theorem}

\section{Leyes de los grandes números}
\subsection*{Ley débil de los grandes números}
Sean $X_1, \dots, X_n$ variables aleatorias en $(\Omega, \mathcal{A}, P)$ y sea $S_n = X_1 + \dots + X_n$.
La sucesión $\{X_n\}_{n \geq 1}$ verifica la ley débil de los grandes números si existen sucesiones numéricas $\{a_n\}$ y $\{b_n\}$ con $b_n \uparrow \infty$ tales que:
$$\frac{S_n - a_n}{b_n} \xrightarrow{p} 0$$

\begin{note}
    Escribir $X_n \to c$ con $c \in \mathbb{R}$ es equivalente a $X_n \to X$ con $X \sim \delta(c)$.
\end{note}

\begin{theorem}[Bernoulli]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes con $X_i \sim Ber(p)$, donde $0 < p < 1$.
    Entonces:
    $$\frac{S_n}{n} \xrightarrow{p} p$$
    Es decir, $\{X_n\}_{n \geq 1}$ verifica la ley débil de los grandes números para $a_n = np$ y $b_n = n$.
\end{theorem}

\begin{example}[Ciclos de permutaciones aleatorias]
    Sea $\Omega_n$ el conjunto de permutaciones de $n$ elementos, consideramos la permutación $\pi \in \Omega_9$ dada por:
    \begin{center}
        \begin{tabular}{ c | c c c c c c c c c}
            $i$      & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
            $\pi(i)$ & 3 & 9 & 6 & 8 & 2 & 1 & 5 & 4 & 7
        \end{tabular}
    \end{center}
    Observamos que $\pi$ tiene tres ciclos y se puede escribir como $(1 \; 3 \; 6)(2 \; 9 \; 7 \; 5)(4 \; 8)$.

    Tomando una permutación al azar de $\Omega_n$, queremos estudiar cuántos ciclos tendrá.
    Definimos las variables:
    $$X_{n, k} = \begin{cases}
            1 & \text{si se cierra un ciclo tras el número en la posición } k \\
            0 & \text{en otro caso}
        \end{cases}$$
    En el caso de $\pi$ tenemos que $x_{9, 3} = x_{9, 7} = x_{9, 9} = 1$, con $x_{9, m} = 0$ en el resto.

    Observamos que $S_n = X_{n, 1} + \dots + X_{n, n}$ es el número de ciclos.
    Se puede demostrar que $X_{n, 1}, \dots, X_{n, n}$ son variables aleatorias independientes y que:
    $$P(X_{n, k} = 1) = \frac{1}{n-k+1}$$
    Calculemos su esperanza:
    $$E(S_n) = E(X_{n, 1}) + \dots + E(X_{n, n}) = \frac{1}{n} + \frac{1}{n-1} + \dots + \frac{1}{2} + 1 = \sum_{j=1}^n \frac{1}{j}$$
    Podemos aproximar:
    $$\sum_{j=1}^n \frac{1}{j} \sim \int_1^n \frac{1}{x}dx = \log(n)$$

    Por tanto, sean $b_n = \log(n)$ y $a_n = E(S_n) = \log(n)$, entonces:
    $$\frac{S_n - \log(n)}{\log(n)} \xrightarrow{p} 0 \Leftrightarrow \frac{S_n}{\log(n)} \xrightarrow{p} 1$$
\end{example}

\begin{example}[Polinomios de Bernstein]
    Sea $f$ continua en $[0, 1]$.
    Para cada $x \in [0, 1]$, el polinomio de Bernstein de grado $n$ asociado a $f$ es:
    $$f_n(x) = \sum_{m=0}^n \binom{n}{m}x^m(1-x)^{n-m}f\left(\frac{m}{n}\right)$$
    Sean $X_i \sim Ber(p)$ para $i \geq 1$ con $0 < p < 1$.
    Entonces $S_n = X_1 + \dots + X_n \sim Bi(n, p)$, con:
    $$P(S_n = k) = \binom{n}{k}p^k(1-p)^{n-k}$$
    Por la ley débil de los grandes números de Bernoulli:
    $$\frac{S_n}{n} \xrightarrow{p} p \Leftrightarrow \frac{S_n}{n} - p \xrightarrow{p} 0$$
\end{example}

\begin{theorem}[Chebyshev]
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independientes con media $\mu$ y varianza $\sigma^2$ constantes.
    Entonces:
    $$\frac{S_n}{n} \xrightarrow{p} \mu$$
\end{theorem}

\begin{theorem}[Chebyshev]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con varianza acotada por una constante $c$.
    Entonces:
    $$\frac{S_n - E(S_n)}{n} \xrightarrow{p} 0$$
\end{theorem}

\begin{theorem}[Márkov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias con $V\left(\frac{S_n}{n}\right) \xrightarrow[n \to \infty]{} 0$.
    Entonces:
    $$\frac{S_n - E(S_n)}{n} \xrightarrow{p} 0$$
\end{theorem}

\begin{theorem}[Khinchin]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con media $\mu < \infty$.
    Entonces:
    $$\frac{S_n}{n} \xrightarrow{p} \mu$$
\end{theorem}

\begin{example}
    Sean $X_1, X_2, \dots$ variables aleatorias independientes e idénticamente distribuidas con $X_j \sim U([0, 1])$ para todo $j$.
    Sea $f: [0, 1] \to \mathbb{R}$ una función medible en $[0, 1]$ tal que:
    $$\int_0^1 |f(x)|dx < \infty$$
    Consideramos las variables aleatorias $f(X_1), f(X_2), \dots$.
    Observamos que:
    $$E(f(X_j)) = \int_0^1 f(x)dx < \infty$$
    Por la ley débil de los grandes números:
    $$\frac{1}{n}(f(X_1) + \dots + f(X_n)) \xrightarrow{p} \int_0^1 f(x)dx$$
\end{example}

\begin{example}
    Sean $X_1, X_2, \dots$ variables aleatorias independientes e idénticamente distribuidas con valores en $\{1, \dots, n\}$ con $n$ fijo, donde $X_i$ es el valor del dato $i$-ésimo.
    Definimos la variable $\tau_k^n = \inf \{m \leq n : \#\{X_1, \dots, X_m\} = k\}$, que representa el instante en el cual obtenemos $k$ datos distintos.
    Es claro que $\tau_1^n = 1$ y asumimos $\tau^n_0 = 0$.

    Definimos también $X_{n, k} = \tau^n_k - \tau^n_{k-1}$, para $1 \leq k \leq n$, que indica el tiempo en conseguir el $k$-ésimo dato distinto.
    Observamos que:
    $$X_{n, k} \sim Ge\left(\frac{n-(k-1)}{n}\right)$$
    Por tanto:
    $$E(X_{n, k}) = \frac{n}{n-(k-1)}, \quad V(X_{n, k}) = \left(\frac{n}{n-(k-1)}\right)^2$$

    Por último, definimos $T_n = \sum_{k=1}^n X_{n, k} = \tau^n_n$, que es el tiempo en completar la colección.
    \begin{align*}
        E(T_n) & = \sum_{k=1}^n \frac{n}{n-(k-1)} = n \sum_{m=1}^n \frac{1}{m} \sim n\log(n)                                                                     \\
        V(T_n) & = \sum_{k=1}^n \left(\frac{n}{n-(k-1)}\right)^2 = n^2 \sum_{m=1}^n \frac{1}{m^2} \leq n^2 \sum_{m=1}^\infty \frac{1}{m^2} = n^2 \frac{\pi^2}{6}
    \end{align*}
    Tomamos $a_n = E(T_n)$ y $b_n = n\log(n)$.
    Como $\frac{V(T_n)}{b_n} \to 0$ y se verifica la ley débil de los grandes números, entonces:
    $$\frac{T_n - n\log(n)}{n\log(n)} \xrightarrow{p} 0 \text{, es decir, } \frac{T_n}{n\log(n)} \xrightarrow{p} 1$$
    Si $n = 365$, entonces el tiempo en completar la colección será aproximadamente $T_n \sim 365\log(365) > 2153$.
\end{example}

\subsection*{Ley fuerte de los grandes números}
\begin{definition}
    Una sucesión $\{X_n\}_{n \geq 1}$ de una variable aleatoria verifica la ley fuerte de los grandes números si existen sucesiones numéricas $\{a_n\}$ y $\{b_n\}$ con $b_n \uparrow \infty$ tales que:
    $$\frac{S_n - a_n}{b_n} \xrightarrow{cs} 0$$
    donde $S_n = \sum_{i=1}^n X_i$.
\end{definition}

\begin{remark}
    Estudiaremos el caso $a_n = E(S_n)$, $b_n = n$.
    Queremos estudiar la convergencia de series de variables aleatorias, como $\sum_{i=1}^\infty X_n$ y $\sum_{i=1}^\infty (X_n - E(X_n))$.
    Diremos que una serie $\sum_{i=1}^\infty$ converge casi seguro para indicar que $\sum_{i=1}^\infty X_n < \infty$ en casi todo punto.
\end{remark}

\begin{lemma}
    Sea $\{X_n\}$ una sucesión de variables aleatorias.
    Entonces $\sum_{n=1}^\infty X_n$ converge casi seguro si y solo si:
    $$\lim\limits_{n \to \infty} \lim\limits_{m \to \infty} P(\max_{1 \leq j \leq m} |S_j - S_n| \geq \varepsilon) = 0, \quad \forall \varepsilon > 0$$
\end{lemma}

\begin{theorem}[Criterio de convergencia de Kolmogórov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con $\sum_{n=1}^\infty V(X_n) < \infty$.
    Entonces $\sum_{n=1}^\infty (X_n - E(X_n)) < \infty$ casi seguro.
\end{theorem}

\begin{remark}
    Si $\sum_{n=1}^\infty (X_n - E(X_n)) < \infty$ casi seguro, entonces:
    $$\sum_{n=1}^\infty X_n < \infty \text{ casi seguro } \Leftrightarrow \sum_{n=1}^\infty E(X_n) < \infty$$
\end{remark}

\begin{theorem}[Recíproco del criterio de convergencia de Kolmogórov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes.
    Si existe una constante $c > 0$ tal que $\{X_n\} \leq c$ casi seguro para todo $n \geq 1$, entonces:
    $$\sum_{n=1}^\infty (X_n - E(X_n)) < \infty \text{ casi seguro } \Leftrightarrow \sum_{n=1}^\infty V(X_n) < \infty$$
\end{theorem}

\begin{corollary}
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes tales que $\{X_n\} \leq c$ para alguna constante $c > 0$.
    Si $\sum_{n=1}^\infty X_n < \infty$ casi seguro, entonces también convergen casi seguro las series:
    $$\sum_{n=1}^\infty (X_n - E(X_n)), \quad \sum_{n=1}^\infty E(X_n), \quad \sum_{n=1}^\infty V(X_n)$$
\end{corollary}

\begin{theorem}[Condición suficiente de Kolmogórov]
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con varianza finita.
    Si $\sum_{n=1}^\infty \frac{V(X_n)}{n^2} < \infty$ entonces $\{X_n\}$ verifica la ley fuerte de los grandes números, es decir,
    $$\frac{S_n - E(S_n)}{n} \xrightarrow{cs} 0$$
\end{theorem}

\begin{lemma}[Kronecker]
    Sean $\{X_n\}$ una sucesión de variables aleatorias y $\{a_n\}$ una sucesión de números reales con $a_n \uparrow \infty$.
    Si $\sum_{n=1}^\infty \frac{X_n}{a_n} < \infty$ casi seguro, entonces:
    $$\frac{1}{a_n} \sum_{k=1}^n X_k \xrightarrow{cs} 0$$
\end{lemma}

\begin{definition}
    Sean $\{X_n\}$ una sucesión de variables aleatorias y $\{c_n\}$ una sucesión de reales no negativos.
    Se define la sucesión de las variables aleatorias truncadas como $\{Y_n\}_{n \geq 1}$, donde:
    $$Y_n = X_n I_{\{|X_n| < c_n\}}$$
\end{definition}

\begin{definition}
    Dos sucesiones de variables aleatorias $\{X_n\}$ e $\{Y_n\}$ son equivalentes en convergencia cuando:
    $$\sum_{n=1}^\infty P(X_n \neq Y_n) < \infty$$
\end{definition}

\begin{theorem}
    Si $\{X_n\}$ e $\{Y_n\}$ son equivalentes en convergencia, entonces se verifican:
    \begin{enumerate}
        \item $P(\limsup\limits_{n \to \infty} \{X_n \neq Y_n\}) = 0$
        \item $\sum_{n=1}^\infty X_n < \infty \text{ casi seguro } \Leftrightarrow \sum_{n=1}^\infty Y_n < \infty \text{ casi seguro}$
        \item $\frac{1}{n} \sum_{k=1}^n (X_k - Y_k) \xrightarrow[n \to \infty]{cs} 0$
    \end{enumerate}
\end{theorem}

\begin{remark}
    Si elegimos $c_n = c$ constante para todo $n \in \mathbb{N}$,
    $$Y_n = X_n I_{\{|X_n| < c\}} = \begin{cases}
            X_n & \text{si } |X_n| < c \\
            0   & \text{en otro caso}
        \end{cases}$$
    Entonces:
    $$\sum_{n=1}^\infty P(X_n \neq Y_n) = \sum_{n=1}^\infty P(|X_n| \geq c)$$
    Si existe $c$ tal que esa serie es finita, tenemos equivalencia entre $\{X_n\}$ e $\{Y_n\}$.
\end{remark}

\begin{example}
    Sea $\{X_n\}$ una sucesión de variables aleatorias independientes con medida de probabilidad inducida:
    $$P_{X_n}(a) = \begin{cases}
            \frac{1}{2}\left(1-\frac{1}{n^2}\right) & \text{si } a = -1, 1     \\
            \frac{1}{2n^2}                          & \text{si } a = -e^n, e^n \\
            0                                       & \text{en el resto}
        \end{cases}$$
    Observamos que $X_n$ es discreta, con $sop(X_n) = \{-e^n, -1, 1, e^n\}$.

    Queremos ver que $\{X_n\}$ verifica la ley fuerte de los grandes números.
    Sin embargo:
    $$\sum_{n=1}^\infty \frac{V(X_n)}{n^2} = \sum_{n=1}^\infty \left(\frac{1}{n^2} - \frac{1}{n^4} + \frac{e^{2n}}{n^4}\right) = \infty$$
    El problema es que las variables no tienen $E(X_n^2) < \infty$.
    Para solucionarlo, consideramos las variables truncadas:
    $$Y_n = X_nI_{\{-1, 1\}} = X_n I_{\{|X_n| < 1 + \varepsilon\}}$$
    Observamos que:
    $$\sum_{n=1}^\infty P(X_n \neq -1, 1) = \sum_{n=1}^\infty P(X_n = e^n, X_n = e^{-n}) = \sum_{n=1}^\infty \frac{1}{n^2} < \infty$$
\end{example}

\begin{theorem}[Tres series de Kolmogórov]
    Sean $\{X_n\}$ una sucesión de variables aleatorias independientes y $\{X_n^c\}$ la sucesión de variables aleatorias truncadas por alguna constante $c > 0$.
    Supongamos que las tres siguientes series convergen:
    $$\sum_{n=1}^\infty P(X_n \neq X_n^c), \quad \sum_{n=1}^\infty E(X_n^c), \quad \sum_{n=1}^\infty V(X_n^c)$$
    Entonces $\sum_{n=1}^\infty X_n$ converge casi seguro.

    Recíprocamente, si $\sum_{n=1}^\infty X_n < \infty$ casi seguro, entonces las tres series convergen para todo $c > 0$.
\end{theorem}

\begin{lemma}
    Sea $X$ una variable aleatoria.
    Entonces:
    $$E(|X|) < \infty \Leftrightarrow \sum_{n=1}^\infty P(|X| \geq n) < \infty$$
    Además:
    $$\sum_{n=1}^\infty P(|X| \geq n) \leq E(|X|) \leq 1 + \sum_{n=1}^\infty P(|X| \geq n)$$
\end{lemma}

\begin{theorem}[Ley fuerte de los grandes números]
    Sean $X_1, X_2, \dots$ variables aleatorias independientes e idénticamente distribuidas con $E(X_i) = \mu < \infty$ para todo $i$.
    Sea $S_n = X_1 + \dots + X_n$.
    Entonces:
    $$\frac{S_n}{n} \xrightarrow{cs} \mu$$
    Recíprocamente, si $\frac{S_n}{n} \xrightarrow{cs} c$, con $c$ constante, entonces $E(X_i) = c$ para todo $i$.
\end{theorem}

\section{Teorema central del límite}
\begin{theorem}[Teorema central del límite de Lindeberg-Lévy]
    Sean variables aleatorias $X_1, X_2, \dots$ independientes e idénticamente distribuidas con media $E(X_i) = \mu < \infty$ y varianza $V(X_i) = \sigma^2 < \infty$ para todo $i$.
    Sean $S_n = X_1 + \dots + X_n$.
    Entonces:
    $$\frac{S_n-n\mu}{\sigma\sqrt{n}} = \frac{\sqrt{n}}{\sigma}\left(\frac{S_n}{n}-\mu\right) \xrightarrow{d} Z \sim N(0, 1)$$
\end{theorem}

\begin{remark}
    En particular, si $\mu = 0$ y $\sigma^2 = 1$, entonces:
    $$\frac{S_n}{\sqrt{n}} \xrightarrow{d} Z \sim N(0, 1)$$
\end{remark}

\begin{exercise}
    Sean $X_n \sim U([0, \pi])$ variables aleatorias independientes e idénticamente distribuidas.
    Sea $S_n = \sum_{j=1}^n \sin(X_j)$.
    Queremos encontrar sucesiones $\{a_n\}$ y $\{b_n\}$ tales que:
    $$\frac{S_n-a_n}{b_n} \xrightarrow{d} Z \sim N(0, 1)$$

    Definimos $Y_j = \sin(X_j)$, de forma que $S_n = \sum_{j=1}^n Y_j$.
    Veamos si $E(Y_j)$ y $V(Y_j)$ son constantes para aplicar el teorema central del límite de Lindeberg-Lévy.
    \begin{align*}
        E(Y_j)   & = \int_0^\pi \sin(x)f(x)dx = \frac{1}{\pi} \int_0^\pi \sin(x)dx = \frac{2}{\pi} \\
        E(Y_j^2) & = \frac{1}{\pi} \int_0^\pi \sin^2(x)dx = \frac{1}{2}
    \end{align*}
    Como $E(Y_j) = \frac{2}{\pi}$ y $V(Y_j) = \frac{1}{2} - \left(\frac{2}{\pi}\right)^2$ son constantes, los $X_n$ verifican el teorema central del límite de Lindeberg-Lévy para:
    $$a_n = n\mu = \frac{2n}{\pi}, \quad b_n = \sqrt{n\left(\frac{1}{2}-\frac{4}{\pi^2}\right)}$$
\end{exercise}

\begin{theorem}[Teorema central del límite de Lindeberg-Feller]
    Sean variables aleatorias $X_1, X_2, \dots$ independientes con $E(X_n) = \mu_n < \infty$ y $V(X_n) = \sigma^2_n < \infty$.
    Sea $S_n = X_1 + \dots + X_n$ y sea $s_n = V(S_n) = \sum_{j=1}^n \sigma_j$.
    Entonces:
    \begin{enumerate}
        \item $\frac{1}{s_n} \sum_{j=1}^n (X_j-\mu_j) \xrightarrow{d} Z \sim N(0, 1)$
        \item $\max_{1 \leq j \leq n} \frac{\sigma^2_j}{s_n^2} \xrightarrow[n \to \infty]{} 0$
    \end{enumerate}
    es equivalente a:
    $$L_n(\varepsilon) = \frac{1}{s_n^2} \sum_{j=1}^n \int_{|x-\mu_j| > \varepsilon s_n} (x-\mu_j)^2dF_j(x) \xrightarrow[n \to \infty]{} 0, \quad \forall \varepsilon > 0$$
\end{theorem}