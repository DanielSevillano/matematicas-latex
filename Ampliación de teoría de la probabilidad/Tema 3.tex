\chapter{Convergencia}
Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad, con $P: \mathcal{A} \to [0, 1]$.
Estudiaremos las sucesiones $\{A_n\}_{n \geq 1}$ con $A_i \in \mathcal{A}$ para todo $i \geq 1$.

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    \begin{itemize}
        \item Definimos el límite superior de la sucesión como:
              $$\limsup\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} \bigcup_{m \geq n} A_m \in \mathcal{A}$$
        \item Definimos el límite inferior de la sucesión como:
              $$\liminf\limits_{n \to \infty} A_n = \bigcup_{n \geq 1} \bigcap_{m \geq n} A_m \in \mathcal{A}$$
    \end{itemize}
\end{definition}

\begin{remark}
    $$\liminf\limits_{n \to \infty} A_n \subseteq \limsup\limits_{n \to \infty} A_n$$
\end{remark}

La sucesión $\{A_n\}_n$ converge si:
$$\liminf\limits_{n \to \infty} A_n = \limsup\limits_{n \to \infty} A_n = \lim\limits_{n \to \infty} A_n$$

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    $\{A_n\}_n$ es monótona creciente si:
    $$A_1 \subseteq A_2 \subseteq \dots \subseteq A_n \subseteq \dots$$

    En ese caso,
    $$\lim\limits_{n \to \infty} A_n = \bigcup_{n \geq 1} A_n$$
\end{definition}

\begin{definition}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    $\{A_n\}_n$ es monótona decreciente si:
    $$A_1 \supseteq A_2 \supseteq \dots \supseteq A_n \supseteq \dots$$

    En ese caso,
    $$\lim\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} A_n$$
\end{definition}

\begin{theorem}
    Sea $\{A_n\}_n \subset \mathcal{A}$ monótona.
    Entonces:
    $$P(\lim\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(A_n)$$
\end{theorem}

\begin{theorem}
    Sea $\{A_n\}_n \subset \mathcal{A}$.
    Entonces:
    \begin{enumerate}
        \item $$P(\limsup\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(\bigcup_{m \geq n} A_m)$$
        \item $$P(\liminf\limits_{n \to \infty} A_n) = \lim\limits_{n \to \infty} P(\bigcap_{m \geq n} A_m)$$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Sean $\{A_n\}_n \subset \mathcal{A}$ y $\omega \in \Omega$.
    Entonces:
    \begin{enumerate}
        \item $w \in \limsup\limits_{n \to \infty} A_n$ si y solo si existe una sucesión de índices
              $$n_1 < n_2 < \dots < n_k < \dots$$
              tal que $w \in A_{n_k}$, para $k = 1, 2, \dots$.
        \item $w \in \liminf\limits_{n \to \infty} A_n$ si y solo si existe $n_0 \geq 1$ tal que $w \in A_m$ para todo $m \geq n_0$.
    \end{enumerate}
\end{theorem}

\begin{theorem}[Primer lema de Borel-Cantelli]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_n \subset \mathcal{A}$ tal que $\sum_{n \geq 1} P(A_n) < \infty$.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 0$$
\end{theorem}

Veamos que el recíproco del primer lema de Borel-Cantelli no es cierto.
\begin{example}
    Sea $(\Omega, \mathcal{A}, P)$ el espacio de probabilidad dado por $\Omega = (0, 1)$, $A = \mathcal{B}_\Omega$ y $P$ la medida de Lebesgue.
    Consideramos la sucesión $\{A_n\}_n$ con $A_n = \left\{\left(0, \frac{1}{\sqrt{n}}\right)\right\}$.
    Observamos que:
    $$\limsup\limits_{n \to \infty} A_n = \bigcap_{n \geq 1} \bigcup_{m \geq n} \left(0, \frac{1}{\sqrt{n}}\right) = \emptyset \Rightarrow P(\limsup\limits_{n \to \infty} A_n) = 0$$
    Sin embargo,
    $$\sum_{n=1}^\infty P(A_n) = \sum_{i=1}^\infty \frac{1}{\sqrt{n}} = \infty$$
\end{example}

\begin{remark}
    \begin{align*}
        \limsup\limits_{n \to \infty} I_{A_n}(\omega) & = I_{\limsup\limits_{n \to \infty} A_n}(\omega) \\
        \liminf\limits_{n \to \infty} I_{A_n}(\omega) & = I_{\liminf\limits_{n \to \infty} A_n}(\omega)
    \end{align*}
\end{remark}

\begin{theorem}[Segundo lema de Borel-Cantelli]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_n \subset \mathcal{A}$ con $A_i$ independientes tal que $\sum_{n \geq 1} P(A_n) = \infty$.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 1$$
\end{theorem}

\begin{remark}
    $$P(\liminf\limits_{n \to \infty} A_n) \leq \liminf\limits_{n \to \infty} P(A_n) \leq \limsup\limits_{n \to \infty} P(A_n) \leq P(\limsup\limits_{n \to \infty} A_n)$$
\end{remark}

\begin{example}
    Un mono pulsando teclas al azar sobre un teclado durante un periodo de tiempo infinito escribirá el Quijote y cualquier texto un número infinito de veces.
\end{example}

\begin{corollary}[Ley 0-1 de Borel-Cantelli]
    Sea $\{A_n\}_n \subset \mathcal{A}$ con $A_i$ independientes.
    Entonces:
    $$P(\limsup\limits_{n \to \infty} A_n) = 0 \quad \text{o} \quad P(\limsup\limits_{n \to \infty} A_n) = 1$$
\end{corollary}

\section{Tipos de convergencia}
Sea $X$ variable aleatoria en el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, definimos:
$$\Omega_1 = \{ \omega \in \Omega : \liminf\limits_{n \to \infty} X_n(\omega) = \limsup\limits_{n \to \infty} X_n(\omega) \}$$

Recordamos que:
\begin{itemize}
    \item Sean $X: \Omega \to S$ y $f: S \to T$ dos funciones medibles.
          Entonces $f(X)$ es medible.
    \item Sean $X_1, \dots, X_n$ variables aleatorias y $f: \mathbb{R}^n \to \mathbb{R}$ una función.
          Entonces $f(X_1, \dots, X_n)$ es variable aleatoria.
    \item Toda función continua es medible Borel.
\end{itemize}

\begin{theorem}
    Sea ${X_n}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Entonces son variables aleatorias:
    \begin{itemize}
        \item $\inf X_n$
        \item $\sup X_n$
        \item $\liminf\limits_{n \to \infty} X_n$
        \item $\limsup\limits_{n \to \infty} X_n$
    \end{itemize}
\end{theorem}

\begin{corollary}
    $\Omega_1$ es medible.
\end{corollary}

\begin{definition}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge casi seguro si $P(\Omega_1) = 1$.

    En tal caso escribimos $X_n \xrightarrow{cs} X$, con $X = \limsup\limits_{n \to \infty} X_n$.
\end{definition}

\begin{theorem}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias independientes en $(\Omega, \mathcal{A}, P)$.
    Entonces las siguientes afirmaciones son equivalentes:
    \begin{enumerate}
        \item $X_n \xrightarrow{cs} X$.
        \item $P(\liminf\limits_{n \to \infty} Y_{n, k}) = 1$ para todo $k \geq 1$.
        \item $P(\limsup\limits_{n \to \infty} Y_{n, k}^c) = 0$ para todo $k \geq 1$.
    \end{enumerate}
    donde
    $$Y_{n, k} = \left\{ \omega \in \Omega : |X_n(\omega) - X(\omega)| < \frac{1}{k} \right\}$$
\end{theorem}

\begin{example}
    Veamos un contrajemplo para el caso en el que no hay independencia.

    Sea $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = \begin{cases}
            1 & \text{si } 0 \leq \frac{1}{n}          \\
            0 & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Veamos que las variables aleatorias no son independientes dos a dos:
    \begin{align*}
        P(X_n = 1 | X_{n-1} = 0) & = \frac{P((X_n = 1) \cap (X_{n-1} = 0))}{P(X_{n-1} = 0)} =                             \\
                                 & = \frac{m\left([0, \frac{1}{n}] \cap (\frac{1}{n-1}, 1]\right)}{1 - \frac{1}{n-1}} = 0 \\
        P(X_n = 1)               & = \frac{1}{n}
    \end{align*}
    Calculamos el límite de la sucesión:
    $$\lim\limits_{n \to \infty} X_n(\omega) = \begin{cases}
            1 & \text{si } \omega = 0        \\
            0 & \text{si } 0 < \omega \leq 1
        \end{cases}$$
    Sea $X(\omega) = 0$, $X_n \xrightarrow{cs} X$ porque:
    $$P(\{\lim\limits_{n \to \infty} X_n \neq X\}) = P(\{0\}) = 0$$
    Sea $\varepsilon > 0$,
    $$Y_{n, 1/\varepsilon}^c = \left\{ \omega \in [0, 1] : |X_n(\omega) - 0| \geq \varepsilon \right\}$$
    Sin embargo, observamos que:
    $$\sum_{n=1}^\infty P(Y_{n, 1/\varepsilon}^c) = \sum_{n=1}^\infty P(|X_n| \geq \varepsilon) = \sum_{n=1}^\infty P(X_n = 1) = \sum_{n=1}^\infty \frac{1}{n} = \infty$$
\end{example}

\begin{definition}
    Sea $\{X_n\}_{n \geq 1}$ una sucesión de variables aleatorias en $(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge en probabilidad a $X$ si para todo $\varepsilon > 0$
    $$P(Y_{n, \varepsilon}) \xrightarrow[n \to \infty]{} 1$$

    En tal caso escribimos $X_n \xrightarrow{p} X$.
\end{definition}

\begin{theorem}
    El límite en probabilidad es único en casi todo punto.
\end{theorem}

\begin{theorem}
    Si $X_n \xrightarrow{cs} X$, entonces $X_n \xrightarrow{p} X$.
\end{theorem}

\begin{example}
    Sea $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = \omega^n$$
    Calculamos el límite de la sucesión:
    $$\lim\limits_{n \to \infty} X_n(\omega) = \begin{cases}
            0 & \text{si } 0 \leq \omega < 1 \\
            1 & \text{si } \omega = 1
        \end{cases}$$
    Sea $X(\omega) = 0$, $X_n \xrightarrow{cs} X$ porque:
    $$P(\{\lim\limits_{n \to \infty} X_n \neq X\}) = P(\{1\}) = 0$$

    Como $X_n$ converge casi seguro, sabemos que $X_n$ converge en probabilidad.
    Veamos que esto es cierto.

    Sea $\varepsilon > 0$,
    $$Y_{n, \varepsilon} = \{|X_n - X| < \varepsilon\} = \{X_n < \varepsilon\}$$
    Observamos que:
    $$F_n(x) = P(X_n \leq x) = \begin{cases}
            0           & \text{si } x < 0        \\
            \sqrt[n]{x} & \text{si } 0 \leq x < 1 \\
            1           & \text{si } x \geq 1
        \end{cases}$$
    Por tanto, si tomamos $\varepsilon \leq 1$,
    $$P(Y_{n, \varepsilon}) = F_n(\varepsilon^-) = \sqrt[n]{\varepsilon} \xrightarrow[n \to \infty]{} 1$$
\end{example}

\begin{example}
    Sean $X_n$ variables aleatorias independientes de Bernoulli en un espacio de medida $(\Omega, \mathcal{A}, P)$.
    $$X_n(\omega) = \begin{cases}
            1 & \text{si hay éxito en la prueba } n    \\
            0 & \text{si no hay éxito en la prueba } n
        \end{cases}$$
    Observamos que:
    $$P(X_n = x) = \begin{cases}
            \frac{1}{n}     & \text{si } x = 1    \\
            1 - \frac{1}{n} & \text{si } x = 0    \\
            0               & \text{en otro caso}
        \end{cases}$$

    Sea $X = 0$, veamos que $X_n$ no converge casi seguro a $X$.
    Sea $\varepsilon > 0$,
    $$\sum_{n=1}^\infty P(Y_{n, \varepsilon}^c) = \sum_{n=1}^\infty P(X_n \geq \varepsilon) = \sum_{n=1}^\infty \frac{1}{n} = \infty$$
    Veamos que aún así $X_n$ converge en probabilidad a $X$.
    $$P(Y_{n, \varepsilon}^c) = P(X_n \geq \varepsilon) = \begin{cases}
            \frac{1}{n} & \text{si } 0 < \varepsilon \leq 1 \\
            0           & \text{si } \varepsilon > 1
        \end{cases} \Rightarrow \lim\limits_{n \to \infty} P(Y_{n, \varepsilon}^c) = 0 \quad \forall \varepsilon$$
    Por tanto, $X_n \xrightarrow{p} X$.
\end{example}

\begin{theorem}
    Si $X_n \xrightarrow{p} X$, entonces existe una subsucesión $\{X_{n_k}\}$ tal que $X_{n_k} \xrightarrow[k \to \infty]{cs} X$.
\end{theorem}

\begin{theorem}
    $X_n \xrightarrow{p} X$ si y solo si toda subsucesión contiene una subsucesión convergente casi seguro.
\end{theorem}

\begin{theorem}
    La convergencia en probabilidad implica la convergencia en distribución.
\end{theorem}

\begin{theorem}
    Sean $X_n$ y $X$ en $(\Omega, \mathcal{A}, P)$ con $X \sim \delta(c)$ y $c$ constante.
    Entonces la convergencia en probabilidad es equivalente a la convergencia en distribución.
\end{theorem}

\subsection*{Convergencia en $L^p$}
Dado $0 < p < \infty$, definimos:
$$L^p(\Omega, \mathcal{A}, P) = \{X: \Omega \to \mathbb{R} : E(|X|^p) < \infty\}$$

\begin{note}
    Los elementos de $L^p$ son en realidad clases de equivalencia, con la relación de equivalencia dada por:
    $$X \sim Y \Leftrightarrow X = Y \text{ en casi todo punto}$$
\end{note}

\begin{note}
    Si $X$ es una variable aleatoria que verifica $E(|X|^p) < \infty$, se dice que es $p$-integrable.
    Esta condición es equivalente a que:
    $$\int_\Omega |X|^p dP < \infty$$
\end{note}

Para $1 \leq p < \infty$ podemos definir la norma:
$$\|X\|_p = \left(\int_\Omega |X|^p dP\right)^{1/p}$$
Si $0 < p < 1$, $\|.\|$ es una pseudonorma.

Esta norma induce una métrica:
\begin{align*}
    d: L^p \times L^p & \to \mathbb{R}^+ \\
    d(X, Y)           & = \|X-Y\|_p
\end{align*}

\begin{definition}
    Sean $X_n, X \in L^p(\Omega, \mathcal{A}, P)$.
    Decimos que $X_n$ converge a $X$ en $L^p$ si:
    $$\|X_n - X\|_p \xrightarrow[n \to \infty]{} 0$$
    Equivalentemente, si:
    $$E(|X_n - X|^p) \xrightarrow[n \to \infty]{} 0$$

    En tal caso escribimos $X_n \xrightarrow{L^p} X$.
\end{definition}

\begin{example}
    Sean $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = n I_{[0, \frac{1}{n}]}(\omega) = \begin{cases}
            n & \text{si } 0 \leq \omega \leq \frac{1}{n} \\
            0 & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Veamos si $X_n$ converge a $X$ en $L^p$.
    \begin{align*}
        E(|X_n - X|^p) & = E(|X_n|^p) = 0^pP(X_n = 0) + n^pP(X_n = n) =                                                                                  \\
                       & = n^pm\left(\left[0, \frac{1}{n}\right]\right) = \frac{n^p}{n} = n^{p-1} \xrightarrow[n \to \infty]{} \begin{cases}
                                                                                                                                   0      & \text{si } p < 1 \\
                                                                                                                                   1      & \text{si } p = 1 \\
                                                                                                                                   \infty & \text{si } p > 1
                                                                                                                               \end{cases}
    \end{align*}
    Luego $X_n$ converge a $X$ en $L^p$ si $p < 1$.
\end{example}

\begin{example}
    Sean $\Omega = [0, 1]$, $\mathcal{A} = \mathcal{B}_{[0, 1]}$ y $P = m$ la medida de Lebesgue.
    Consideramos:
    $$X_n(\omega) = 2^n I_{[0, \frac{1}{n}]}(\omega) = \begin{cases}
            2^n & \text{si } 0 \leq \omega \leq \frac{1}{n} \\
            0   & \text{si } \frac{1}{n} < \omega \leq 1
        \end{cases}$$
    Sea $X \sim \delta(0)$, se puede comprobar que $X_n \xrightarrow{p} X$.
    Veamos si $X_n$ converge a $X$ en $L^p$.
    \begin{align*}
        E(|X_n - X|^p) & = E(|X_n|^p) = 0^pP(X_n = 0) + 2^{np}P(X_n = 2^n) =                                                      \\
                       & = 2^{np}m\left(\left[0, \frac{1}{n}\right]\right) = \frac{2^{np}}{n} \xrightarrow[n \to \infty]{} \infty
    \end{align*}
    Luego $X_n$ no converge a $X$ en $L^p$.
\end{example}

\begin{proposition}[Desigualdad de Márkov]
    Sean $X$ no negativa y $a > 0$.
    Entonces:
    $$P(X \geq a) \leq \frac{E(X)}{a}$$
    Si $X \in L^p$,
    $$P(X \geq a) \leq \frac{E(X^p)}{a^p}$$
\end{proposition}

\begin{remark}
    Si $X \in L^p$ cualquiera, $|X|$ es no negativa luego:
    $$P(|X| \geq a) \leq \frac{E(|X|^p)}{a^p}$$
\end{remark}

\begin{theorem}
    Sean $X_n, X \in L^p$, con $0 < p < \infty$.
    Si $X_n \xrightarrow{L^p} X$, entonces $X_n \xrightarrow{p} X$.
\end{theorem}

\begin{theorem}
    El límite en $L^p$ es único.
\end{theorem}

\begin{theorem}
    Si $X_n \xrightarrow{p} X$ con $X_n, X \in L^p$ y existe $Y \in L^p$ tal que $|X_n| \leq Y$ para todo $n \in \mathbb{N}$, entonces $X_n \xrightarrow{L^p} X$.
\end{theorem}

\section{Leyes de los grandes números}
\subsection*{Ley débil de los grandes números}
Sean $X_1, \dots, X_n$ variables aleatorias en $(\Omega, \mathcal{A}, P)$ y sea $S_n = X_1 + \dots + X_n$.
La sucesión $\{X_n\}_{n \geq 1}$ verifica la ley débil de los grandes números si existen sucesiones numéricas $\{a_n\}$ y $\{b_n\}$ con $b_n \uparrow \infty$ tales que:
$$\frac{S_n - a_n}{b_n} \xrightarrow{p} 0$$

\begin{note}
    Escribir $X_n \to c$ con $c \in \mathbb{R}$ es equivalente a $X_n \to X$ con $X \sim \delta(c)$.
\end{note}

\begin{theorem}[Bernoulli]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes con $X_i \sim Ber(p)$, donde $0 < p < 1$.
    Entonces:
    $$\frac{S_n}{n} \xrightarrow{p} p$$
    Es decir, $\{X_n\}_{n \geq 1}$ verifica la ley débil de los grandes números para $a_n = np$ y $b_n = n$.
\end{theorem}

\begin{example}[Ciclos de permutaciones aleatorias]
    Sea $\Omega_n$ el conjunto de permutaciones de $n$ elementos, consideramos la permutación $\pi \in \Omega_9$ dada por:
    \begin{center}
        \begin{tabular}{ c | c c c c c c c c c}
            $i$      & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
            $\pi(i)$ & 3 & 9 & 6 & 8 & 2 & 1 & 5 & 4 & 7
        \end{tabular}
    \end{center}
    Observamos que $\pi$ tiene tres ciclos y se puede escribir como $(1 \; 3 \; 6)(2 \; 9 \; 7 \; 5)(4 \; 8)$.

    Tomando una permutación al azar de $\Omega_n$, queremos estudiar cuántos ciclos tendrá.
    Definimos las variables:
    $$X_{n, k} = \begin{cases}
            1 & \text{si se cierra un ciclo tras el número en la posición } k \\
            0 & \text{en otro caso}
        \end{cases}$$
    En el caso de $\pi$ tenemos que $x_{9, 3} = x_{9, 7} = x_{9, 9} = 1$, con $x_{9, m} = 0$ en el resto.

    Observamos que $S_n = X_{n, 1} + \dots + X_{n, n}$ es el número de ciclos.
    Se puede demostrar que $X_{n, 1}, \dots, X_{n, n}$ son variables aleatorias independientes y que:
    $$P(X_{n, k} = 1) = \frac{1}{n-k+1}$$
    Calculemos su esperanza:
    $$E(S_n) = E(X_{n, 1}) + \dots + E(X_{n, n}) = \frac{1}{n} + \frac{1}{n-1} + \dots + \frac{1}{2} + 1 = \sum_{j=1}^n \frac{1}{j}$$
    Podemos aproximar:
    $$\sum_{j=1}^n \frac{1}{j} \sim \int_1^n \frac{1}{x}dx = \log(n)$$

    Por tanto, sean $b_n = \log(n)$ y $a_n = E(S_n) = \log(n)$, entonces:
    $$\frac{S_n - \log(n)}{\log(n)} \xrightarrow{p} 0 \Leftrightarrow \frac{S_n}{\log(n)} \xrightarrow{p} 1$$
\end{example}

\begin{example}[Polinomios de Bernstein]
    Sea $f$ continua en $[0, 1]$.
    Para cada $x \in [0, 1]$, el polinomio de Bernstein de grado $n$ asociado a $f$ es:
    $$f_n(x) = \sum_{m=0}^n \binom{n}{m}x^m(1-x)^{n-m}f\left(\frac{m}{n}\right)$$
    Sean $X_i \sim Ber(p)$ para $i \geq 1$ con $0 < p < 1$.
    Entonces $S_n = X_1 + \dots + X_n \sim Bi(n, p)$, con:
    $$P(S_n = k) = \binom{n}{k}p^k(1-p)^{n-k}$$
    Por la ley débil de los grandes números de Bernoulli:
    $$\frac{S_n}{n} \xrightarrow{p} p \Leftrightarrow \frac{S_n}{n} - p \xrightarrow{p} 0$$
\end{example}